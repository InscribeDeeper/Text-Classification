{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c27450e",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Package-import\" data-toc-modified-id=\"Package-import-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Package import</a></span></li><li><span><a href=\"#Data-Loading\" data-toc-modified-id=\"Data-Loading-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data Loading</a></span></li><li><span><a href=\"#Preprocessing\" data-toc-modified-id=\"Preprocessing-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Preprocessing</a></span><ul class=\"toc-item\"><li><span><a href=\"#parsing\" data-toc-modified-id=\"parsing-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>parsing</a></span><ul class=\"toc-item\"><li><span><a href=\"#typo_parser\" data-toc-modified-id=\"typo_parser-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>typo_parser</a></span></li><li><span><a href=\"#email_address_parser\" data-toc-modified-id=\"email_address_parser-3.1.2\"><span class=\"toc-item-num\">3.1.2&nbsp;&nbsp;</span>email_address_parser</a></span></li><li><span><a href=\"#bytedata_parser\" data-toc-modified-id=\"bytedata_parser-3.1.3\"><span class=\"toc-item-num\">3.1.3&nbsp;&nbsp;</span>bytedata_parser</a></span></li><li><span><a href=\"#structure_parser\" data-toc-modified-id=\"structure_parser-3.1.4\"><span class=\"toc-item-num\">3.1.4&nbsp;&nbsp;</span>structure_parser</a></span></li><li><span><a href=\"#reference_parser\" data-toc-modified-id=\"reference_parser-3.1.5\"><span class=\"toc-item-num\">3.1.5&nbsp;&nbsp;</span>reference_parser</a></span></li></ul></li><li><span><a href=\"#main-structural_email\" data-toc-modified-id=\"main-structural_email-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>main structural_email</a></span></li></ul></li><li><span><a href=\"#Main-block\" data-toc-modified-id=\"Main-block-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Main block</a></span></li><li><span><a href=\"#Saved-processed-data\" data-toc-modified-id=\"Saved-processed-data-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Saved processed data</a></span></li><li><span><a href=\"#module-test\" data-toc-modified-id=\"module-test-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>module test</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001d7ee9",
   "metadata": {},
   "source": [
    "## Package import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "750c8e3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-03T07:10:41.118287Z",
     "start_time": "2021-12-03T07:10:40.405285Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import mailparser\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75663bee",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "- from file into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bad85141",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-03T07:10:43.365975Z",
     "start_time": "2021-12-03T07:10:41.120289Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded folder under ../../data/train: \n",
      "\n",
      "../../data/train\\alt.atheism\n",
      "../../data/train\\comp.graphics\n",
      "../../data/train\\comp.os.ms-windows.misc\n",
      "../../data/train\\comp.sys.ibm.pc.hardware\n",
      "../../data/train\\comp.sys.mac.hardware\n",
      "../../data/train\\comp.windows.x\n",
      "../../data/train\\misc.forsale\n",
      "../../data/train\\rec.autos\n",
      "../../data/train\\rec.motorcycles\n",
      "../../data/train\\rec.sport.baseball\n",
      "../../data/train\\rec.sport.hockey\n",
      "../../data/train\\sci.crypt\n",
      "../../data/train\\sci.electronics\n",
      "../../data/train\\sci.med\n",
      "../../data/train\\sci.space\n",
      "../../data/train\\soc.religion.christian\n",
      "../../data/train\\talk.politics.guns\n",
      "../../data/train\\talk.politics.mideast\n",
      "../../data/train\\talk.politics.misc\n",
      "../../data/train\\talk.religion.misc\n",
      "\n",
      "Loaded folder under ../../data/test: \n",
      "\n",
      "../../data/test\\alt.atheism\n",
      "../../data/test\\comp.graphics\n",
      "../../data/test\\comp.os.ms-windows.misc\n",
      "../../data/test\\comp.sys.ibm.pc.hardware\n",
      "../../data/test\\comp.sys.mac.hardware\n",
      "../../data/test\\comp.windows.x\n",
      "../../data/test\\misc.forsale\n",
      "../../data/test\\rec.autos\n",
      "../../data/test\\rec.motorcycles\n",
      "../../data/test\\rec.sport.baseball\n",
      "../../data/test\\rec.sport.hockey\n",
      "../../data/test\\sci.crypt\n",
      "../../data/test\\sci.electronics\n",
      "../../data/test\\sci.med\n",
      "../../data/test\\sci.space\n",
      "../../data/test\\soc.religion.christian\n",
      "../../data/test\\talk.politics.guns\n",
      "../../data/test\\talk.politics.mideast\n",
      "../../data/test\\talk.politics.misc\n",
      "../../data/test\\talk.religion.misc\n"
     ]
    }
   ],
   "source": [
    "def load_data_folder(path):\n",
    "    \"\"\"\n",
    "    @param folders: the train or test directory\n",
    "    @return: document list with [doc_path, doc, label, original_idx]\n",
    "    \"\"\"\n",
    "    folders = glob(path+\"/**\")  # explore all the folder under the directory\n",
    "\n",
    "    docs = []\n",
    "    for classes in folders:\n",
    "        label = classes.split(\"\\\\\")[-1]\n",
    "        doc_paths = glob(classes+\"\\\\**\")\n",
    "        \n",
    "        for doc_path in doc_paths:\n",
    "            original_idx = doc_path.split(\"\\\\\")[-1]\n",
    "            \n",
    "            with open(doc_path, encoding=\"UTF-8\") as f:\n",
    "                text = f.read()\n",
    "            docs.append([doc_path, text, label, original_idx])\n",
    "\n",
    "    print(f\"\\nLoaded folder under {path}: \\n\")\n",
    "    for folder in folders:\n",
    "        print(folder)\n",
    "        \n",
    "    return docs\n",
    "\n",
    "\n",
    "corpus_train_docs = load_data_folder(path=\"../../data/train\")\n",
    "corpus_test_docs = load_data_folder(path=\"../../data/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7382e307",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cc4803",
   "metadata": {},
   "source": [
    "### parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d381bd4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-03T07:10:43.391976Z",
     "start_time": "2021-12-03T07:10:43.367975Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original_idx duplicate count: 1060  on  11083\n",
      "original_idx duplicate count: 770  on  7761\n"
     ]
    }
   ],
   "source": [
    "corpus_train = pd.DataFrame(corpus_train_docs, columns=[\"doc_path\", \"text\", \"label\", \"original_idx\"])\n",
    "corpus_train = corpus_train.reset_index().rename(columns={\"index\":\"global_index\"})\n",
    "\n",
    "corpus_test = pd.DataFrame(corpus_test_docs, columns=[\"doc_path\", \"text\", \"label\", \"original_idx\"])\n",
    "corpus_test = corpus_test.reset_index().rename(columns={\"index\":\"global_index\"})\n",
    "\n",
    "print(\"original_idx duplicate count:\", corpus_train.shape[0] - corpus_train.original_idx.drop_duplicates().shape[0], \" on \", corpus_train.shape[0])\n",
    "print(\"original_idx duplicate count:\", corpus_test.shape[0] - corpus_test.original_idx.drop_duplicates().shape[0], \" on \", corpus_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff76e38",
   "metadata": {},
   "source": [
    "#### typo_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a064decd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-03T07:10:43.399981Z",
     "start_time": "2021-12-03T07:10:43.394976Z"
    }
   },
   "outputs": [],
   "source": [
    "def typo_parser(x):\n",
    "    \"\"\"\n",
    "    1. replace irrelevant symbol \"|\" or \"*\"\n",
    "    2. remove extra space \"  \"\n",
    "    3. replace extra \\n \"\\n\\n\" into \"\\n\"\n",
    "    4. replace \"> *>\" into \">>\" for further analysis\n",
    "\n",
    "    @param string: email body string\n",
    "    @return: cleaned email body string, extracted emails\n",
    "    \n",
    "    # test_string = 'www.\\n com\\n\\n or ?\\n>\\n    >>\\n    \\n > > >|> (note) \\n> \\n I\\nam not good enough with regex>'\n",
    "    # typo_parser(test_string)\n",
    "\n",
    "    \"\"\"\n",
    "    # x = re.sub('([,:;?!\\.”\\)])\\n', '\\g<1> ', x)  # add space for symbol like .\\n or ?\\n\n",
    "    # x = re.sub('(\\w)\\n(\\w)', '\\g<1> \\g<2>', x)  # add space for symbol like word\\nword\n",
    "    x = re.sub('\\n', ' \\n ', x)  # add space for between \\n\n",
    "    x = re.sub(\"[\\*|\\|\\^]\", \"\", x) # replace irrelevant symbol \"|\" or \"*\"\n",
    "    \n",
    "    x = re.sub(\">[ >]*>\", \">>\", x)# compress > [?] > \n",
    "    x = re.sub(\"\\[.*?\\]\", \"\", x, flags=re.S)  # separate for typo like [a)\n",
    "    x = re.sub(\"\\(.*?\\)\", \"\", x, flags=re.S)\n",
    "\n",
    "    x = re.sub(\"\\n[ \\n]*\\n\", \"\\n\", x) # compress \\n\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d930d0ef",
   "metadata": {},
   "source": [
    "#### email_address_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0537b19f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-03T07:10:43.407976Z",
     "start_time": "2021-12-03T07:10:43.401975Z"
    }
   },
   "outputs": [],
   "source": [
    "def email_address_parser(string):\n",
    "    \"\"\"\n",
    "    extract and remove email from the body\n",
    "    @param string: email body string\n",
    "    @return: cleaned email body string, extracted emails\n",
    "    \"\"\"\n",
    "    emails = None\n",
    "    emails = re.findall(\" ?[\\S]+@[\\S]+ ?\", string)\n",
    "    string = re.sub(\" ?[\\S]+@[\\S]+ ?\", \" \", string)\n",
    "    return string, emails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9c38d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-03T05:07:49.765508Z",
     "start_time": "2021-12-03T05:07:49.762509Z"
    }
   },
   "source": [
    "#### bytedata_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f5bb53b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-03T07:10:43.414977Z",
     "start_time": "2021-12-03T07:10:43.409976Z"
    }
   },
   "outputs": [],
   "source": [
    "def bytedata_parser(string, threshold=50):\n",
    "    \"\"\"\n",
    "    Since 99% of english words length ranged from [1,20], but consider special symbol there, we set the threshold with 50 for only parse bytdata like photo\n",
    "    If length of span larger than threshold, then we will not treat it as a word. \n",
    "    sep can only use space\n",
    "    \"\"\"\n",
    "    bytedata = None\n",
    "    clean_string = \" \".join([word for word in re.split(\" \", string) if len(word)<=threshold])\n",
    "    ## sentence length is the same\n",
    "    # clean_string = \"\\n\".join([word for word in re.split(\"\\n\", clean_string) if len(word)<=threshold])\n",
    "    bytedata = [word for word in re.split(\" \", string) if len(word)>threshold]\n",
    "    return clean_string, bytedata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b50df4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-03T05:07:50.069508Z",
     "start_time": "2021-12-03T05:07:50.067507Z"
    }
   },
   "source": [
    "#### structure_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68bcdf2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-03T07:10:43.423976Z",
     "start_time": "2021-12-03T07:10:43.416977Z"
    }
   },
   "outputs": [],
   "source": [
    "def structure_parser(string):\n",
    "    \"\"\"\n",
    "    @param parser: email string\n",
    "    @return: structural information for email header, body, others\n",
    "    \"\"\"\n",
    "    error_message = None\n",
    "    header = {}\n",
    "    body = \"\"\n",
    "    others = []\n",
    "    try:\n",
    "        mail = mailparser.parse_from_string(string)\n",
    "        if mail.has_defects:  # [first line error]\n",
    "            remove_first_line_string = \"\\n\".join(string.split(\"\\n\")[1:])\n",
    "            mail = mailparser.parse_from_string(remove_first_line_string)\n",
    "            # print(\"remove_first_line_string update for \")\n",
    "        header, body = mail.headers, mail.body\n",
    "        others = [mail.date, mail.delivered_to, mail.to_domains, error_message]\n",
    "\n",
    "    except Exception as error:\n",
    "        error_message = error\n",
    "    return header, body, others"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83861a7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-03T05:07:46.125233Z",
     "start_time": "2021-12-03T05:07:46.123231Z"
    }
   },
   "source": [
    "#### reference_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00488c7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-03T07:10:43.433977Z",
     "start_time": "2021-12-03T07:10:43.425976Z"
    }
   },
   "outputs": [],
   "source": [
    "def extra_parser(x):\n",
    "    \"\"\"\n",
    "    remove_flag and extra space\n",
    "    \"\"\"\n",
    "    x = re.sub(\"(?:In article)?.*writes:\" , \"\", x, flags=re.S)\n",
    "    x = re.sub(\" {2,}\", \" \", x) # compress space\n",
    "    return x\n",
    "\n",
    "def reference_parser(string, match_type=2):\n",
    "    \"\"\"\n",
    "    Consider reply with referencing previous email, we need to separate them to make prediction separately.\n",
    "    @param \n",
    "        string: email body string\n",
    "        match_type: 0 with return only main body, 1 with return main body + previous one reference, 2 with more reference\n",
    "    @return: \n",
    "        reply, previous_one, previous_two in the email\n",
    "    \n",
    "    @ test with the following code\n",
    "    string = \" \\n\\n\\n\\n    >>>zero email \\n\\n >>first email\\n >second email\\n reply email \\n\"\n",
    "    reply, previous_one, previous_two = reference_parser(string, match_type=2)\n",
    "    print(\"## reply\\n\", repr(reply))\n",
    "    print(\"## previous_one\\n\", repr(previous_one))\n",
    "    print(\"## previous_two\\n\", repr(previous_two))\n",
    "    \"\"\"\n",
    "    \n",
    "    previous_one, previous_two, reply = '', '', ''\n",
    "\n",
    "    # extract reply with out containing >\n",
    "    reply = \" \".join([s for s in string.split(\"\\n\") if \">\" not in s])\n",
    "    reply = extra_parser(reply)\n",
    "    \n",
    "    # add \"\\n\" before string to matchign [^>]{1}\n",
    "    if match_type>0:\n",
    "        previous_one = \" \".join(re.findall(\"[^>]{1}>{1}([^>]{1}[\\S ]*)\\n\", \"\\n\" + string)) # matching >\n",
    "        previous_one = extra_parser(previous_one)\n",
    "        \n",
    "    if match_type>1: # flag reference_two\n",
    "        previous_two = \" \".join(re.findall(\"[^>]{1}>{2}([^>]{1}[\\S ]*)\\n\", \"\\n\" + string)) # matching >>\n",
    "        previous_two = extra_parser(previous_two)\n",
    "    # previous_two_more_pt = \"[^>]{1}>{2,}([^>]{1}[\\S ]*)\\n\" # matching >> or >>> more\n",
    "    return reply, previous_one, previous_two"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0278191",
   "metadata": {},
   "source": [
    "### main structural_email "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6b1361e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-03T07:10:43.444976Z",
     "start_time": "2021-12-03T07:10:43.434976Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def structural_email(data, bytedata_parser_threshold=50, reference_parser_match_type=2):\n",
    "    \"\"\"\n",
    "    This is a parser pipeline, parser order matters.\n",
    "    1. string => structure email to separate => header, body, others\n",
    "    2. body => remove typo and some irrelevant words => body\n",
    "    3. body => parse and remove email from body => body_no_email\n",
    "    4. body_no_email => parse and remove binary data like BMP or picture from body => body_no_binary_no_email\n",
    "    5. body_no_binary_no_email => separate email reference and reply => reply, previous_one, previous_two\n",
    "    \n",
    "    @param data: data text series including all the training set or test set\n",
    "    @return: structural information\n",
    "    \"\"\"\n",
    "    print(\"Preprocessing for unstructure email...\")\n",
    "    header_info = []\n",
    "    body_info = []\n",
    "    others_info = []\n",
    "    for string in tqdm(data):\n",
    "        header, body, others = structure_parser(string)\n",
    "        body = typo_parser(body)\n",
    "        body_no_email, emails = email_address_parser(body)\n",
    "        body_no_binary_no_email, bytedata = bytedata_parser(body_no_email, threshold=bytedata_parser_threshold)\n",
    "        reply, previous_one, previous_two = reference_parser(body_no_binary_no_email, match_type=reference_parser_match_type)\n",
    "\n",
    "        header_info.append(header)\n",
    "        body_info.append([reply, previous_one, previous_two])\n",
    "        others_info.append(others+[emails]+[bytedata])\n",
    "\n",
    "    a1 = pd.DataFrame.from_dict(header_info)\n",
    "    a2 = pd.DataFrame(body_info, columns=[\"reply\", \"reference_one\", \"reference_two\"])\n",
    "    a3 = pd.DataFrame(others_info, columns=[\"date\", \"delivered_to\", \"to_domains\", \"error_message\", \"contained_emails\", \"long_string\"])\n",
    "    structure_email = pd.concat([a1, a2, a3], axis=1)\n",
    "    return structure_email"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3553257",
   "metadata": {},
   "source": [
    "## Main block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8500b350",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-03T07:14:36.386421Z",
     "start_time": "2021-12-03T07:10:43.447978Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                        | 0/11083 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing for unstructure email...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|████████████████████████████████████████▋                                   | 5937/11083 [00:55<00:33, 155.87it/s]Email content 'x-usenet-faq' not handled\n",
      "Email content 'x-usenet-faq' not handled\n",
      "Email content 'x-usenet-faq' not handled\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 11083/11083 [02:18<00:00, 79.79it/s]\n",
      "  0%|▎                                                                              | 27/7761 [00:00<00:29, 264.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing for unstructure email...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 7761/7761 [01:33<00:00, 82.72it/s]\n"
     ]
    }
   ],
   "source": [
    "structural_train = structural_email(corpus_train[\"text\"])\n",
    "structural_test = structural_email(corpus_test[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3cf60360",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-03T07:42:15.655745Z",
     "start_time": "2021-12-03T07:42:15.633340Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['global_index', 'doc_path', 'text', 'label', 'original_idx', 'From', 'Subject', 'Summary', 'Keywords', 'Expires', 'Distribution', 'Organization', 'Supersedes', 'Lines', 'X-Newsreader', 'NNTP-Posting-Host', 'Reply-To', 'Nntp-Posting-Host', 'In-Reply-To', 'News-Software', 'X-Mailer', 'Originator', 'Article-I.D.', 'X-News-Reader', 'X-Sender', 'X-Disclaimer', 'Nntp-Posting-User', 'X-Bytes', 'X-Xxmessage-Id', 'X-Xxdate', 'X-Useragent', 'In-reply-to', 'OD-Comment-To', 'ReplyTo', 'Disclaimer', 'Comments', 'Posting-Front-End', 'X-Reader', 'Mime-Version', 'Content-Type', 'Content-Transfer-Encoding', 'X-UserAgent', 'X-NewsSoftware', 'Nntp-Software', 'Oganization', 'Apparently-To', 'X-Comment-To', 'X-Gateway', 'X-Advert', 'Cc', 'X-News-Software', 'X-Posted-From', 'Follow-Ups', 'X-Auth-User', 'X-FTN-To', 'X-Gated-By', 'X-Standard-Disclaimer', 'Moderator', 'X-XXMessage-ID', 'X-XXDate', 'To', 'Posted-Date', 'Received-Date', 'Orginization', 'X-Md4-Signature', 'Return-Receipt-To', 'X-Mail-Reader', 'Content-Length', 'X-Copyright', 'Original-To', 'X-Received', 'X-To', 'Return-Path', 'Nntp-Posting-Host-[nntpd-23809]', 'Organisation', 'X-Date', 'Nntp-Posting-Host-[nntpd-8755]', 'Nntp-Posting-Host-[nntpd-19510]', 'Nntp-Posting-Host-[nntpd-29970]', 'X-Software', 'X-AltNet-ID', 'MIME-Version', 'Bcc', 'Status', 'Nntp-Posting-Host-[nntpd-681]', 'Weather', 'Moon-Phase', 'X-Last-Updated', 'X-Face', 'X-Maildoor', 'X-Newssoftware', 'Nf-ID', 'Nf-From', 'X-Address', 'X-Fax', 'X-Phone', 'IMPORTANT-INFO', 'X-Added', 'Original-Sender', 'X-Alt.reply-Address', 'X-X-From', 'Mmdf-Warning', 'Followups-to', 'X-Newsposter', 'X-Header', 'X-Cc', 'Oanization', 'reply', 'reference_one', 'reference_two', 'date', 'delivered_to', 'to_domains', 'error_message', 'contained_emails', 'long_string']\n"
     ]
    }
   ],
   "source": [
    "train = pd.concat([corpus_train, structural_train], axis=1)\n",
    "test = pd.concat([corpus_test, structural_test], axis=1)\n",
    "all_cols = train.columns.tolist()\n",
    "print(all_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0e0b80",
   "metadata": {},
   "source": [
    "## Saved processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78e41b15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-03T07:17:57.919222Z",
     "start_time": "2021-12-03T07:17:57.291670Z"
    }
   },
   "outputs": [],
   "source": [
    "train.to_json('../../data/structured_train.json')\n",
    "test.to_json('../../data/structured_test.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d6c949",
   "metadata": {},
   "source": [
    "## module test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32f9d3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-03T07:14:36.545516Z",
     "start_time": "2021-12-03T07:14:36.545516Z"
    }
   },
   "outputs": [],
   "source": [
    "def checking_text(idx, write_in_local=True):\n",
    "    x = train[train[\"global_index\"] == idx]\n",
    "    string = x[\"text\"].iloc[0]\n",
    "    body = x[\"reply\"].iloc[0]\n",
    "    x_path = x[\"doc_path\"].iloc[0]\n",
    "    x_label = x[\"label\"].iloc[0]\n",
    "    \n",
    "    if write_in_local:\n",
    "        with open(\"./module_checking_sample.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(x_label+\"\\n\\n\")\n",
    "            f.write(x_path+\"\\n\\n\")\n",
    "            f.write(string)\n",
    "    return string, body, x_path, x_label\n",
    "\n",
    "\n",
    "module_test = True\n",
    "\n",
    "if module_test:\n",
    "    # 可以分开一个pyfile, 并且把这里的过程保存下来, 然后写在report中\n",
    "    # idx = 22\n",
    "    idx = 9187\n",
    "\n",
    "    string, reply, x_path, x_label = checking_text(idx)\n",
    "\n",
    "    header, body, others = structure_parser(string)\n",
    "    print(\"\\nrepr(header):   \\n\", repr(header))\n",
    "    print(\"\\nrepr(body):   \\n\", repr(body))\n",
    "    print(\"\\nrepr(others):   \\n\", repr(others))\n",
    "\n",
    "    body = typo_parser(body)\n",
    "    print(\"\\nrepr(body):   \\n\", repr(body))\n",
    "\n",
    "    body_no_email, emails = email_address_parser(body)\n",
    "    print(\"\\nrepr(body):   \\n\", repr(body))\n",
    "    print(\"\\nrepr(emails):   \\n\", repr(emails))\n",
    "    print(\"\\nrepr(body_no_email):   \\n\", repr(body_no_email))\n",
    "\n",
    "    body_no_binary_no_email, bytedata = bytedata_parser(body_no_email, threshold=25)\n",
    "    print(\"\\nrepr(bytedata):   \\n\", repr(bytedata))\n",
    "    print(\"\\nrepr(body_no_binary_no_email):   \\n\", repr(body_no_binary_no_email))\n",
    "\n",
    "    reply, previous_one, previous_two = reference_parser(body_no_binary_no_email, match_type=2)\n",
    "\n",
    "    print(\"\\nrepr(reply):   \\n\", repr(reply))\n",
    "    print(\"\\nrepr(previous_one):   \\n\", repr(previous_one))\n",
    "    print(\"\\nrepr(previous_two):   \\n\", repr(previous_two))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "03d78440",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-03T07:35:12.842902Z",
     "start_time": "2021-12-03T07:35:12.829899Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 333.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing for unstructure email...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open('regex_sample.txt','r') as f:\n",
    "    sample = f.read()\n",
    "\n",
    "parsed_f = structural_email(pd.Series(sample))\n",
    "parsed_f.to_json(\"regex_sample_parsed.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "246.773px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
