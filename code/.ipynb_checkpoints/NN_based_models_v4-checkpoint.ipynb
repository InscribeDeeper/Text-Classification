{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a73ce9a4",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#TextCNN\" data-toc-modified-id=\"TextCNN-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>TextCNN</a></span><ul class=\"toc-item\"><li><span><a href=\"#notes:\" data-toc-modified-id=\"notes:-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>notes:</a></span></li></ul></li><li><span><a href=\"#LSTM\" data-toc-modified-id=\"LSTM-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>LSTM</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e726484b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-12-09T02:49:16.498Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from sklearn import metrics\n",
    "from clustering_utils import *\n",
    "from eda_utils import *\n",
    "from myutils_V6 import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "seeds = 2021\n",
    "np.random.seed(seeds)\n",
    "\n",
    "import pandas as pd\n",
    "train = pd.read_json('../data/structured_train.json')\n",
    "test = pd.read_json('../data/structured_test.json')\n",
    "\n",
    "train.sample(frac=1, random_state=seeds).reset_index(drop=True)\n",
    "test.sample(frac=1, random_state=seeds).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# train = train.groupby('label').sample(50, random_state=seeds)\n",
    "# test = test.groupby('label').sample(50, random_state=seeds)\n",
    "\n",
    "####################################\n",
    "### columns selection\n",
    "####################################\n",
    "select_cols = [\"global_index\", \"doc_path\", \"label\",\n",
    "               \"reply\", \"reference_one\", \"reference_two\", \"tag_reply\", \"tag_reference_one\", \"tag_reference_two\",\n",
    "               \"Subject\", \"From\", \"Lines\", \"Organization\", \"contained_emails\", \"long_string\", \"text\", \"error_message\"\n",
    "               ]\n",
    "print(\"\\nmay use cols: \\n\", select_cols)\n",
    "train = train[select_cols]\n",
    "train[[\"reply\", \"reference_one\", \"reference_two\", \"tag_reply\", \"tag_reference_one\", \"tag_reference_two\", \"Subject\", \"From\", \"Lines\", \"Organization\"]] = train[[\"reply\", \"reference_one\", \"reference_two\", \"tag_reply\", \"tag_reference_one\", \"tag_reference_two\", \"Subject\", \"From\", \"Lines\", \"Organization\"]].astype(str)\n",
    "\n",
    "test = test[select_cols]\n",
    "test[[\"reply\", \"reference_one\", \"reference_two\", \"tag_reply\", \"tag_reference_one\", \"tag_reference_two\", \"Subject\", \"From\", \"Lines\", \"Organization\"]] = test[[\"reply\", \"reference_one\", \"reference_two\", \"tag_reply\", \"tag_reference_one\", \"tag_reference_two\", \"Subject\", \"From\", \"Lines\", \"Organization\"]].astype(str)\n",
    "\n",
    "\n",
    "\n",
    "####################################\n",
    "### upsampling for small class\n",
    "####################################\n",
    "group_size = train.groupby('label').size()\n",
    "mean_size = int(group_size.mean())\n",
    "small_groups = group_size[group_size<mean_size].index.tolist()\n",
    "\n",
    "train_small_groups = train[train['label'].isin(small_groups)].groupby('label').sample(n=mean_size, replace=True, random_state=seeds)\n",
    "train_large_groups = train[~train['label'].isin(small_groups)]\n",
    "upsampling_train = pd.concat([train_small_groups, train_large_groups], axis=0)\n",
    "upsampling_group_size = upsampling_train.groupby('label').size()\n",
    "upsampling_info = pd.concat([group_size, upsampling_group_size, upsampling_group_size-group_size], axis=1)\n",
    "upsampling_info.columns = ['before_upsampling', 'after_upsampling', 'increase']\n",
    "\n",
    "train = upsampling_train\n",
    "\n",
    "\n",
    "\n",
    "####################################\n",
    "### string normalized\n",
    "####################################\n",
    "\n",
    "\n",
    "from gensim.utils import tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "def normal_string(x):\n",
    "    x = remove_stopwords(x)\n",
    "#     x = \" \".join(preprocess_string(x))\n",
    "    x = \" \".join(word_tokenize(x, preserve_line=False)).strip()\n",
    "    return x\n",
    "\n",
    "####################################\n",
    "### data augmentation\n",
    "####################################\n",
    "r = train['reply']\n",
    "rr1 = train['reply'] + ' ' + train['reference_one']\n",
    "r1r2 = train['reference_one'] + ' ' + train['reference_two']\n",
    "sr = train['Subject'] + ' ' + train['reply']\n",
    "# train_text = train['tag_reply']\n",
    "# train_text = train['tag_reply']+ ' ' + train['tag_reference_one']\n",
    "select_comb = 2\n",
    "\n",
    "train_text = pd.concat([r, rr1, r1r2, sr][:select_comb],axis=0).reset_index(drop=True).astype(str).apply(lambda x: normal_string(x))\n",
    "train_label = pd.concat([train['label'] for _ in range(select_comb)], axis=0).reset_index(drop=True)\n",
    "\n",
    "####################################\n",
    "### test set preprocessing\n",
    "####################################\n",
    "# test_text  = test['tag_reply'] \n",
    "# test_text  = test['tag_reply'] + ' ' + test['tag_reference_one']\n",
    "test_text  = test['reply'] + ' ' + test['reference_one']\n",
    "test_label = test['label']\n",
    "test_text = test_text.apply(lambda x: normal_string(x))\n",
    "test_text.sample(5, random_state=10).tolist()\n",
    "\n",
    "\n",
    "####################################\n",
    "### label mapper\n",
    "####################################\n",
    "labels = sorted(train_label.unique())\n",
    "label_mapper = dict(zip(labels, range(len(labels))))\n",
    "train_label = train_label.map(label_mapper)\n",
    "test_label = test_label.map(label_mapper)\n",
    "\n",
    "\n",
    "upsampling_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a380c6a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-09T02:48:57.282700Z",
     "start_time": "2021-12-09T02:48:57.282700Z"
    }
   },
   "outputs": [],
   "source": [
    "####################################\n",
    "### hyper params \n",
    "####################################\n",
    "filters = '\"#$%&()*+,-/:;<=>@[\\\\]^_`{|}~\\t\\n0123465789!.?\\''\n",
    "MAX_NB_WORDS_ratio = 0.95\n",
    "MAX_DOC_LEN_ratio = 0.99\n",
    "MAX_NB_WORDS = eda_MAX_NB_WORDS(train_text, ratio=MAX_NB_WORDS_ratio, char_level=False, filters=filters)\n",
    "MAX_DOC_LEN = eda_MAX_DOC_LEN(train_text, ratio=MAX_DOC_LEN_ratio, char_level=False, filters=filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1950c85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-09T02:48:57.284699Z",
     "start_time": "2021-12-09T02:48:57.284699Z"
    }
   },
   "outputs": [],
   "source": [
    "####################################\n",
    "### train val test split\n",
    "####################################\n",
    "x_train_val, y_train_val, x_test, y_test = train_text, train_label, test_text, test_label\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train_val, y_train_val, test_size=0.2, stratify=y_train_val)\n",
    "\n",
    "####################################\n",
    "### preprocessor for NN input\n",
    "####################################\n",
    "processor = text_preprocessor(MAX_DOC_LEN, MAX_NB_WORDS, train_text, filters=filters)\n",
    "x_train = processor.generate_seq(x_train)\n",
    "x_val = processor.generate_seq(x_val)\n",
    "x_test = processor.generate_seq(x_test)\n",
    "# y_train = to_categorical(y_train)\n",
    "# y_val = to_categorical(y_val)\n",
    "# y_test = to_categorical(y_test)\n",
    "\n",
    "\n",
    "print('Shape of x_tr: ' + str(x_train.shape))\n",
    "print('Shape of y_tr: ' + str(y_train.shape))\n",
    "print('Shape of x_val: ' + str(x_val.shape))\n",
    "print('Shape of y_val: ' + str(y_val.shape))\n",
    "print('Shape of x_test: ' + str(x_test.shape))\n",
    "print('Shape of y_test: ' + str(y_test.shape))\n",
    "\n",
    "\n",
    "info = pd.concat([y_train.value_counts(), y_val.value_counts(), y_val.value_counts()/y_train.value_counts(), y_train.value_counts()/y_train.size\\\n",
    "                 , y_test.value_counts(), y_test.value_counts()/y_test.size], axis=1)\n",
    "info.index = labels\n",
    "info.columns = ['tr_size', 'val_size', 'val_ratio', 'tr_prop', 'test_size', 'test_prop']\n",
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a019380f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-09T02:48:57.285700Z",
     "start_time": "2021-12-09T02:48:57.285700Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from gensim.models import word2vec\n",
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, Embedding, Dropout, TimeDistributed\n",
    "from tensorflow.keras.layers import Embedding, Dense, Conv1D, MaxPooling1D, Dropout, Activation, Input, Flatten, Concatenate, Lambda\n",
    "from tensorflow.keras.layers import SimpleRNN, GRU, Bidirectional, LSTM\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk import bigrams, trigrams, ngrams\n",
    "from nltk.corpus import reuters, stopwords\n",
    "from sklearn import preprocessing\n",
    "# from tensorflow.keras.utils.vis_utils import model_to_dot, plot_model\n",
    "from IPython.display import SVG\n",
    "from numpy.random import seed\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import gensim.downloader as api\n",
    "import glob\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk, string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import seaborn as sns\n",
    "import string, os\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e47544a",
   "metadata": {},
   "source": [
    "# TextCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db5da6d",
   "metadata": {},
   "source": [
    "## notes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90736596",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-09T02:48:57.286702Z",
     "start_time": "2021-12-09T02:48:57.286702Z"
    }
   },
   "outputs": [],
   "source": [
    "# define Model for classification\n",
    "\n",
    "def model_Create(FS, NF, EMB, MDL, MNW, PWV=None, optimizer='RMSprop', trainable_switch=True):\n",
    "    cnn_box = cnn_model_l2(FILTER_SIZES=FS, MAX_NB_WORDS=MNW, MAX_DOC_LEN=MDL, EMBEDDING_DIM=EMB,\n",
    "                        NUM_FILTERS=NF, PRETRAINED_WORD_VECTOR=PWV, trainable_switch=trainable_switch)\n",
    "    # Hyperparameters: MAX_DOC_LEN\n",
    "    q1_input = Input(shape=(MDL,), name='q1_input')\n",
    "    encode_input1 = cnn_box(q1_input)\n",
    "    # half_features = int(len(FS)*NF/2)*10\n",
    "    x = Dense(384, activation='relu', name='half_features')(encode_input1)\n",
    "    x = Dropout(rate=0.3, name='dropout1')(x)\n",
    "#     x = Dense(256, activation='relu', name='dense1')(x)\n",
    "#     x = Dropout(rate=0.3, name='dropou2')(x)\n",
    "    x = Dense(128, activation='relu', name='dense2')(x)\n",
    "    x = Dropout(rate=0.3, name='dropout3')(x)\n",
    "    x = Dense(64, activation='relu', name='dense3')(x)\n",
    "    x = Dropout(rate=0.3, name='dropout4')(x)\n",
    "    pred = Dense(20, activation='softmax', name='Prediction')(x)\n",
    "    model = Model(inputs=q1_input, outputs=pred)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "                  metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
    "    return model\n",
    "\n",
    "\n",
    "EMBEDDING_DIM = 200\n",
    "# W2V =  processor.w2v_pretrain(EMBEDDING_DIM, min_count=2, seed=1, cbow_mean=1,negative=5, window=20, workers=7) # pretrain w2v by gensim\n",
    "# W2V = processor.load_glove_w2v(EMBEDDING_DIM) # download glove\n",
    "W2V = None\n",
    "trainable_switch = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9821d103",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-09T02:48:57.288700Z",
     "start_time": "2021-12-09T02:48:57.288700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set hyper parameters\n",
    "FILTER_SIZES = [1, 2, 3, 4, 5]\n",
    "NUM_FILTERS = 32\n",
    "\n",
    "# OPT = optimizers.Adam(learning_rate=0.005)\n",
    "OPT = optimizers.RMSprop(learning_rate=0.001) # 'RMSprop'\n",
    "PWV = W2V\n",
    "model = model_Create(FS=FILTER_SIZES, NF=NUM_FILTERS, EMB=EMBEDDING_DIM,\n",
    "                     MDL=MAX_DOC_LEN, MNW=MAX_NB_WORDS+1, PWV=PWV, \n",
    "                     optimizer=OPT, trainable_switch=trainable_switch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bcd69a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-09T02:48:57.289700Z",
     "start_time": "2021-12-09T02:48:57.289700Z"
    }
   },
   "outputs": [],
   "source": [
    "# visual_textCNN(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebc0600",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-09T02:48:57.290699Z",
     "start_time": "2021-12-09T02:48:57.290699Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHES = 10  # 20步以上\n",
    "patience = 20\n",
    "\n",
    "file_name = 'test'\n",
    "BestModel_Name = file_name + 'Best_GS_2'\n",
    "BEST_MODEL_FILEPATH = BestModel_Name\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_sparse_categorical_accuracy', patience=patience, verbose=1, mode='max') # patience: number of epochs with no improvement on monitor : val_loss\n",
    "checkpoint = ModelCheckpoint(BEST_MODEL_FILEPATH, monitor='val_sparse_categorical_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "history = model.fit(x_train, y_train, validation_data=(x_test,y_test), batch_size=BATCH_SIZE, epochs=NUM_EPOCHES, callbacks=[earlyStopping, checkpoint], verbose=1)\n",
    "# history = model.fit(x_train, y_train, validation_data=(x_val, y_val), batch_size=BATCH_SIZE, epochs=NUM_EPOCHES, callbacks=[earlyStopping, checkpoint], verbose=1)\n",
    "model.load_weights(BestModel_Name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e44226",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-09T02:48:57.292701Z",
     "start_time": "2021-12-09T02:48:57.292701Z"
    }
   },
   "outputs": [],
   "source": [
    "#### classification Report\n",
    "history_plot(history)\n",
    "y_pred = model.predict(x_test)\n",
    "# print(classification_report(y_test, np.argmax(y_pred, axis=1)))\n",
    "print(classification_report(test_label, np.argmax(y_pred, axis=1), target_names=labels))\n",
    "scores = model.evaluate(x_test, y_test, verbose=2)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "print( \"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48420477",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19d925e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915ccd55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d7d925",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5294c7f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e7cd99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e521f106",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d6a108",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3b16b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-09T02:48:57.293704Z",
     "start_time": "2021-12-09T02:48:57.293704Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#### classification Report\n",
    "history_plot(history)\n",
    "y_pred = model.predict(x_test)\n",
    "# print(classification_report(y_test, np.argmax(y_pred, axis=1)))\n",
    "print(classification_report(test_label, np.argmax(y_pred, axis=1), target_names=labels))\n",
    "scores = model.evaluate(x_test, y_test, verbose=2)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "print( \"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bc22df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4db34d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-09T02:48:57.295701Z",
     "start_time": "2021-12-09T02:48:57.295701Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_train)\n",
    "# print(classification_report(y_test, np.argmax(y_pred, axis=1)))\n",
    "print(classification_report(train_label, np.argmax(y_pred, axis=1), target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf7cfac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5995cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156e307e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fbff5a0a",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a176a6d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-09T02:48:57.296699Z",
     "start_time": "2021-12-09T02:48:57.296699Z"
    }
   },
   "outputs": [],
   "source": [
    "# from tensorflow.keras.layers import SpatialDropout1D, GlobalMaxPooling1D, GlobalMaxPooling2D\n",
    "\n",
    "# def model_Create(FS, NF, EMB, MDL, MNW, PWV = None, optimizer='RMSprop', trainable_switch=True):\n",
    "#     model = Sequential()\n",
    "#     model.add(Embedding(input_dim=MNW, output_dim=EMB, embeddings_initializer='uniform', mask_zero=True, input_length=MDL))\n",
    "#     model.add(Flatten())   \n",
    "# #     model.add(GlobalMaxPooling2D()) # downsampling\n",
    "# #     model.add(SpatialDropout1D(0.2))    \n",
    "#     model.add(Dense(1024, activation='relu'))   \n",
    "#     model.add(Dense(512, activation='relu'))   \n",
    "#     model.add(Dense(128, activation='relu'))   \n",
    "#     model.add(Dense(64, activation='relu'))   \n",
    "#     # model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "#     model.add(Dense(20, activation='softmax'))   \n",
    "#     model.compile(optimizer=optimizer,\n",
    "#           loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False), \n",
    "#           metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
    "#     return model\n",
    "\n",
    "# model = model_Create(FS=FILTER_SIZES, NF=NUM_FILTERS, EMB=EMBEDDING_DIM,\n",
    "#                      MDL=MAX_DOC_LEN, MNW=MAX_NB_WORDS+1, PWV=PWV, trainable_switch=trainable_switch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd56494e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-09T02:48:57.297699Z",
     "start_time": "2021-12-09T02:48:57.297699Z"
    }
   },
   "outputs": [],
   "source": [
    "# visual_textCNN(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e1631d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-09T02:48:57.299700Z",
     "start_time": "2021-12-09T02:48:57.299700Z"
    }
   },
   "outputs": [],
   "source": [
    "# EMBEDDING_DIM = 200\n",
    "# # W2V =  processor.w2v_pretrain(EMBEDDING_DIM, min_count=2, seed=1, cbow_mean=1,negative=5, window=20, workers=7) # pretrain w2v by gensim\n",
    "# # W2V = processor.load_glove_w2v(EMBEDDING_DIM) # download glove\n",
    "# trainable_switch = True\n",
    "# W2V = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352e76ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-09T02:48:57.301701Z",
     "start_time": "2021-12-09T02:48:57.301701Z"
    }
   },
   "outputs": [],
   "source": [
    "# BATCH_SIZE = 64\n",
    "# NUM_EPOCHES = 10  # patience=20\n",
    "# patience = 30\n",
    "\n",
    "# BestModel_Name = 'text_CNN.h5'\n",
    "# BEST_MODEL_FILEPATH = BestModel_Name\n",
    "\n",
    "# earlyStopping = EarlyStopping(monitor='val_sparse_categorical_accuracy', patience=patience, verbose=1, mode='max') # patience: number of epochs with no improvement on monitor : val_loss\n",
    "# checkpoint = ModelCheckpoint(BEST_MODEL_FILEPATH, monitor='val_sparse_categorical_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "# history = model.fit(x_train, y_train, validation_split=0.2, batch_size=BATCH_SIZE, epochs=NUM_EPOCHES, callbacks=[earlyStopping, checkpoint], verbose=1)\n",
    "# model.load_weights(BestModel_Name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0453d996",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-09T02:48:57.302699Z",
     "start_time": "2021-12-09T02:48:57.302699Z"
    }
   },
   "outputs": [],
   "source": [
    "# #### classification Report\n",
    "# history_plot(history)\n",
    "# y_pred = model.predict(x_test)\n",
    "# # print(classification_report(y_test, np.argmax(y_pred, axis=1)))\n",
    "# print(classification_report(test_label, np.argmax(y_pred, axis=1), target_names=labels))\n",
    "# scores = model.evaluate(x_test, y_test, verbose=2)\n",
    "# print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "# print( \"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a094bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1dcb7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "190.458px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
