{"cells":[{"cell_type":"markdown","id":"d89dfe26","metadata":{"id":"d89dfe26","toc":true},"source":["<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n","<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Colab\" data-toc-modified-id=\"Colab-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Colab</a></span></li><li><span><a href=\"#Import\" data-toc-modified-id=\"Import-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Import</a></span></li><li><span><a href=\"#Loading\" data-toc-modified-id=\"Loading-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Loading</a></span><ul class=\"toc-item\"><li><span><a href=\"#extra-one-hot-features\" data-toc-modified-id=\"extra-one-hot-features-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>extra one-hot features</a></span></li><li><span><a href=\"#extra-keywords\" data-toc-modified-id=\"extra-keywords-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>extra keywords</a></span></li></ul></li><li><span><a href=\"#BERT\" data-toc-modified-id=\"BERT-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>BERT</a></span></li></ul></div>"]},{"cell_type":"markdown","id":"05c43715","metadata":{"id":"05c43715"},"source":["<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n","<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#TextCNN\" data-toc-modified-id=\"TextCNN-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>TextCNN</a></span><ul class=\"toc-item\"><li><span><a href=\"#notes:\" data-toc-modified-id=\"notes:-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>notes:</a></span></li></ul></li><li><span><a href=\"#LSTM\" data-toc-modified-id=\"LSTM-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>LSTM</a></span></li></ul></div>"]},{"cell_type":"markdown","id":"3e4f5360","metadata":{"id":"3e4f5360"},"source":["# Colab"]},{"cell_type":"code","execution_count":1,"id":"85be328c","metadata":{"ExecuteTime":{"end_time":"2021-12-10T02:09:53.148780Z","start_time":"2021-12-10T02:09:53.144780Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"85be328c","outputId":"9af579af-b823-46c8-d4db-73208afa38e0","scrolled":true,"executionInfo":{"status":"ok","timestamp":1639131964377,"user_tz":300,"elapsed":8849,"user":{"displayName":"Wei Yang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgKFRGCPHBXOIreYY1GgMyIUPCGnToBiaki3_u3=s64","userId":"18154280958911480081"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Requirement already satisfied: pyLDAvis in /usr/local/lib/python3.7/dist-packages (3.3.1)\n","Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.0)\n","Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.3.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.4.1)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.16.0)\n","Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (3.6.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.0.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.1.0)\n","Requirement already satisfied: funcy in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.16)\n","Requirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (2.7.3)\n","Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.21.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (57.4.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (2.11.3)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyLDAvis) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyLDAvis) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyLDAvis) (1.15.0)\n","Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->pyLDAvis) (5.2.1)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->pyLDAvis) (2.0.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyLDAvis) (3.0.0)\n","Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n","Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n","Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n","Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n","Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.4)\n","Requirement already satisfied: pandas==1.3.0 in /usr/local/lib/python3.7/dist-packages (1.3.0)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.3.0) (1.21.4)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.3.0) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.3.0) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas==1.3.0) (1.15.0)\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import os\n","os.chdir(\"/content/drive/MyDrive/Text-Classification/code\")\n","!pip install pyLDAvis\n","!pip install gensim\n","!pip install pandas==1.3.0"]},{"cell_type":"markdown","id":"e2c39454","metadata":{"id":"e2c39454"},"source":["# Import "]},{"cell_type":"code","execution_count":2,"id":"7ea8c845","metadata":{"ExecuteTime":{"end_time":"2021-12-10T02:09:58.418780Z","start_time":"2021-12-10T02:09:53.150782Z"},"id":"7ea8c845","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639131967066,"user_tz":300,"elapsed":2698,"user":{"displayName":"Wei Yang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgKFRGCPHBXOIreYY1GgMyIUPCGnToBiaki3_u3=s64","userId":"18154280958911480081"}},"outputId":"b6159eb4-c763-48c1-9b8b-1f658a80fe89"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/past/types/oldstr.py:5: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n","  from collections import Iterable\n"]},{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Package words is already up-to-date!\n"]}],"source":["from classification_utils import *\n","from clustering_utils import *\n","from eda_utils import *\n","from nn_utils_keras import *\n","from feature_engineering_utils import *\n","from data_utils import *\n","import warnings \n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"markdown","id":"83f27bb1","metadata":{"id":"83f27bb1"},"source":["# Loading"]},{"cell_type":"code","execution_count":3,"id":"f88921e6","metadata":{"ExecuteTime":{"start_time":"2021-12-10T02:11:34.258Z"},"id":"f88921e6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639131969089,"user_tz":300,"elapsed":2026,"user":{"displayName":"Wei Yang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgKFRGCPHBXOIreYY1GgMyIUPCGnToBiaki3_u3=s64","userId":"18154280958911480081"}},"outputId":"ab58d1d0-5b68-4797-f29b-253226598d30"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","may use cols: \n"," ['global_index', 'doc_path', 'label', 'reply', 'reference_one', 'reference_two', 'tag_reply', 'tag_reference_one', 'tag_reference_two', 'Subject', 'From', 'Lines', 'Organization', 'contained_emails', 'long_string', 'text', 'error_message']\n","combination 1 train:  ['reply', 'reference_one', 'reference_two']\n"]}],"source":["train, test = load_data(only_stem_voc=False, sample50=False)\n","# train, upsampling_info = upsampling_train(train)\n","\n","train_text, train_label = train_augmentation(train, select_comb=[['reply', 'reference_one', 'reference_two']])\n","# train_text, train_label = train_augmentation(train, select_comb=[['text'], ['reply', 'reference_one', 'reference_two']])\n","# train_text, train_label = train['text'], train['label']\n","# test_text, test_label = test['text'], test['label']\n","test_text, test_label = test[['reply', 'reference_one', 'reference_two']].apply(lambda x: \" \".join(x), axis=1), test['label']\n","\n","\n","# test_text = test_text.apply(lambda x: extract_stem_voc(x))\n","# train_text = train_text.apply(lambda x: extract_stem_voc(x))\n","# train_text.to_csv(\"stem_voc_train.csv\")\n","# test_text.to_csv(\"stem_voc_test.csv\")\n","\n","# train_text, test_text = load_stem_voc()"]},{"cell_type":"code","execution_count":4,"id":"9c089c98","metadata":{"ExecuteTime":{"start_time":"2021-12-10T02:09:53.146Z"},"id":"9c089c98","scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639131969090,"user_tz":300,"elapsed":7,"user":{"displayName":"Wei Yang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgKFRGCPHBXOIreYY1GgMyIUPCGnToBiaki3_u3=s64","userId":"18154280958911480081"}},"outputId":"4b1eec30-9553-4e73-ed00-ed621fcbcca2"},"outputs":[{"output_type":"stream","name":"stdout","text":["(11083,)\n","(7761,)\n","(11083,)\n","(7761,)\n","['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"]}],"source":["####################################\n","### label mapper\n","####################################\n","labels = sorted(train_label.unique())\n","label_mapper = dict(zip(labels, range(len(labels))))\n","train_label = train_label.map(label_mapper)\n","test_label = test_label.map(label_mapper)\n","y_train = train_label\n","y_test = test_label\n","\n","print(train_text.shape)\n","print(test_text.shape)\n","print(train_label.shape)\n","print(test_label.shape)\n","print(labels)"]},{"cell_type":"code","execution_count":5,"id":"859a125b","metadata":{"ExecuteTime":{"start_time":"2021-12-10T02:09:53.147Z"},"id":"859a125b","executionInfo":{"status":"ok","timestamp":1639131969090,"user_tz":300,"elapsed":6,"user":{"displayName":"Wei Yang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgKFRGCPHBXOIreYY1GgMyIUPCGnToBiaki3_u3=s64","userId":"18154280958911480081"}}},"outputs":[],"source":["####################################\n","### hyper params \n","####################################\n","# filters = '\"#$%&()*+,-/:;<=>@[\\\\]^_`{|}~\\t\\n0123465789!.?\\''\n","# MAX_NB_WORDS_ratio = 0.95\n","# MAX_DOC_LEN_ratio = 0.99\n","# MAX_NB_WORDS = eda_MAX_NB_WORDS(train_text, ratio=MAX_NB_WORDS_ratio, char_level=False, filters=filters)\n","# MAX_DOC_LEN = eda_MAX_DOC_LEN(train_text, ratio=MAX_DOC_LEN_ratio, char_level=False, filters=filters)"]},{"cell_type":"code","source":["# X_train, X_test, word_to_idx, tfidf_vect = tfidf_vectorizer(train_text, test_text, stop_words=True, binary=True, min_df=5)\n","# X_train, transform_mapper = dimension_reduction(X_train, out_dim=1000) # not allow negative \n","# X_test = transform_mapper.transform(X_test)"],"metadata":{"id":"JnQpGP-FR6x9","executionInfo":{"status":"ok","timestamp":1639131969091,"user_tz":300,"elapsed":6,"user":{"displayName":"Wei Yang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgKFRGCPHBXOIreYY1GgMyIUPCGnToBiaki3_u3=s64","userId":"18154280958911480081"}}},"id":"JnQpGP-FR6x9","execution_count":6,"outputs":[]},{"cell_type":"markdown","id":"fd1183b7","metadata":{"id":"fd1183b7"},"source":["## extra one-hot features"]},{"cell_type":"code","execution_count":7,"id":"b51f5083","metadata":{"ExecuteTime":{"start_time":"2021-12-10T02:09:53.239Z"},"id":"b51f5083","scrolled":true,"executionInfo":{"status":"ok","timestamp":1639131969091,"user_tz":300,"elapsed":6,"user":{"displayName":"Wei Yang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgKFRGCPHBXOIreYY1GgMyIUPCGnToBiaki3_u3=s64","userId":"18154280958911480081"}}},"outputs":[],"source":["# one_hot_X_train, one_hot_X_test, one_hot_word_to_idx, one_hot_count_vect = count_vectorizer(\n","#     train['Subject']+\" \" + train['Organization'], test['Subject']+\" \" + test['Organization'], stop_words=True, binary=False, min_df=3, max_df=0.001)"]},{"cell_type":"markdown","id":"269d56d2","metadata":{"ExecuteTime":{"end_time":"2021-12-10T01:43:16.717443Z","start_time":"2021-12-10T01:43:16.706441Z"},"id":"269d56d2"},"source":["## extra keywords"]},{"cell_type":"code","execution_count":8,"id":"a50842dc","metadata":{"ExecuteTime":{"start_time":"2021-12-10T02:09:53.275Z"},"id":"a50842dc","executionInfo":{"status":"ok","timestamp":1639131969092,"user_tz":300,"elapsed":7,"user":{"displayName":"Wei Yang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgKFRGCPHBXOIreYY1GgMyIUPCGnToBiaki3_u3=s64","userId":"18154280958911480081"}}},"outputs":[],"source":["# label_docs = train.groupby('label')['text'].apply(lambda x: \" \".join(x)) # 要去除标点符号\n","# dtm, _, label_word_to_idx, _ = count_vectorizer(label_docs, [''], stop_words=True, min_df=1, binary=True)\n","# label_idx_to_word = dict([val, key] for key, val in label_word_to_idx.items())\n","# keywords_threshold = 1\n","# keywords_idx = np.where(dtm.sum(axis=0)<=keywords_threshold)[0]\n","# print(\" keywords_idx shape: \")\n","# voc = [label_idx_to_word[idx] for idx in keywords_idx]\n","\n","# keywords_X_train, keywords_X_test, keywords_word_to_idx, keywords_count_vect = count_vectorizer(\n","#     train['text'], test['text'], voc=voc, stop_words=True, min_df=1, binary=True)"]},{"cell_type":"code","execution_count":9,"id":"b9a938ba","metadata":{"ExecuteTime":{"start_time":"2021-12-10T02:09:53.312Z"},"id":"b9a938ba","executionInfo":{"status":"ok","timestamp":1639131969092,"user_tz":300,"elapsed":7,"user":{"displayName":"Wei Yang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgKFRGCPHBXOIreYY1GgMyIUPCGnToBiaki3_u3=s64","userId":"18154280958911480081"}}},"outputs":[],"source":["# _X_train = np.hstack([X_train])\n","# _X_test = np.hstack([X_test])\n","# _X_train = np.hstack([X_train, one_hot_X_train, keywords_X_train])\n","# _X_test = np.hstack([X_test, one_hot_X_test, keywords_X_test])"]},{"cell_type":"markdown","id":"2dc232d5","metadata":{"id":"2dc232d5"},"source":["# BERT"]},{"cell_type":"code","execution_count":10,"id":"d8350e74","metadata":{"ExecuteTime":{"end_time":"2021-12-10T02:40:11.072229Z","start_time":"2021-12-10T02:40:11.065229Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"d8350e74","executionInfo":{"status":"ok","timestamp":1639131975482,"user_tz":300,"elapsed":6396,"user":{"displayName":"Wei Yang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgKFRGCPHBXOIreYY1GgMyIUPCGnToBiaki3_u3=s64","userId":"18154280958911480081"}},"outputId":"032dadbb-745d-4d5f-8d25-7f85dea22d38"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.13.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.2.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: torchinfo in /usr/local/lib/python3.7/dist-packages (1.5.4)\n","Append path:  /content/drive/My Drive/Text-Classification/code/bert_utils\n","My Linux system:  Linux\n","using:  cuda\n","My Linux system:  Linux\n","using:  cuda\n"]}],"source":["!pip install transformers\n","!pip install torchinfo\n","import torch\n","import torch.nn.functional as F\n","\n","from torchinfo import summary\n","from transformers import BertTokenizer, BertModel, AdamW, BertConfig, get_linear_schedule_with_warmup\n","\n","from bert_utils.training_utils import extract_contextual_embedding, train_multi_label_model, model_eval\n","from bert_utils.data_loader import  data_loader_BERT\n","from bert_utils import glovar \n","from bert_utils.model import *"]},{"cell_type":"code","source":["train_one_hot_labels = F.one_hot(torch.tensor(y_train.values))\n","test_one_hot_labels = F.one_hot(torch.tensor(test_label.values))"],"metadata":{"id":"aRfQroUrnRGp","executionInfo":{"status":"ok","timestamp":1639131975482,"user_tz":300,"elapsed":11,"user":{"displayName":"Wei Yang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgKFRGCPHBXOIreYY1GgMyIUPCGnToBiaki3_u3=s64","userId":"18154280958911480081"}}},"id":"aRfQroUrnRGp","execution_count":11,"outputs":[]},{"cell_type":"code","execution_count":12,"id":"621349ed","metadata":{"ExecuteTime":{"end_time":"2021-12-10T02:40:15.647445Z","start_time":"2021-12-10T02:40:13.586174Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"621349ed","executionInfo":{"status":"ok","timestamp":1639131983249,"user_tz":300,"elapsed":7777,"user":{"displayName":"Wei Yang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgKFRGCPHBXOIreYY1GgMyIUPCGnToBiaki3_u3=s64","userId":"18154280958911480081"}},"outputId":"fe2f3854-1e2a-4b5a-8976-b8199ccdf53b"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["cuda:0\n"]},{"output_type":"execute_result","data":{"text/plain":["===========================================================================\n","Layer (type:depth-idx)                             Param #\n","===========================================================================\n","BertModel                                          --\n","├─BertEmbeddings: 1-1                              --\n","│    └─Embedding: 2-1                              23,440,896\n","│    └─Embedding: 2-2                              393,216\n","│    └─Embedding: 2-3                              1,536\n","│    └─LayerNorm: 2-4                              1,536\n","│    └─Dropout: 2-5                                --\n","├─BertEncoder: 1-2                                 --\n","│    └─ModuleList: 2-6                             --\n","│    │    └─BertLayer: 3-1                         7,087,872\n","│    │    └─BertLayer: 3-2                         7,087,872\n","│    │    └─BertLayer: 3-3                         7,087,872\n","│    │    └─BertLayer: 3-4                         7,087,872\n","│    │    └─BertLayer: 3-5                         7,087,872\n","│    │    └─BertLayer: 3-6                         7,087,872\n","│    │    └─BertLayer: 3-7                         7,087,872\n","│    │    └─BertLayer: 3-8                         7,087,872\n","│    │    └─BertLayer: 3-9                         7,087,872\n","│    │    └─BertLayer: 3-10                        7,087,872\n","│    │    └─BertLayer: 3-11                        7,087,872\n","│    │    └─BertLayer: 3-12                        7,087,872\n","├─BertPooler: 1-3                                  --\n","│    └─Linear: 2-7                                 590,592\n","│    └─Tanh: 2-8                                   --\n","===========================================================================\n","Total params: 109,482,240\n","Trainable params: 109,482,240\n","Non-trainable params: 0\n","==========================================================================="]},"metadata":{},"execution_count":12}],"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","bert_model = BertModel.from_pretrained(\"bert-base-uncased\", output_attentions = True, output_hidden_states = True)\n","# global device\n","device = glovar.device_type\n","bert_model = bert_model.to(device)\n","print(next(bert_model.parameters()).device)  # 输出：cpu\n","summary(bert_model)"]},{"cell_type":"code","execution_count":14,"id":"d9f6a91a","metadata":{"id":"d9f6a91a","executionInfo":{"status":"ok","timestamp":1639132007689,"user_tz":300,"elapsed":155,"user":{"displayName":"Wei Yang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgKFRGCPHBXOIreYY1GgMyIUPCGnToBiaki3_u3=s64","userId":"18154280958911480081"}}},"outputs":[],"source":["load_embed= False\n","\n","learning_rate = 0.005\n","epochs = 80\n","patience = 60\n","MAX_DOC_LEN = 128\n","batch_size=32\n","\n","\n","max_len = min(512, MAX_DOC_LEN)\n","label_size = 20\n","label_cols = labels\n","embed_dim = 768\n","hidden_units = 64\n","num_filters = 30\n","kernel_sizes = [1,2,3]\n","embed_type=3"]},{"cell_type":"code","source":["if load_embed is True:\n","    train_sentences_encoding = np.load('train_sentences_encoding.npy')# , mmap_mode='r')\n","    train_input_ids = np.load('train_input_ids.npy')#, mmap_mode='r')\n","    train_sentences_encoding = torch.tensor(train_sentences_encoding)\n","    train_input_ids = torch.tensor(train_input_ids)\n","else:\n","    train_input_ids, train_sentences_encoding = extract_contextual_embedding(train_text, tokenizer, bert_model, max_len = max_len, low_RAM_inner_batch=True, embed_type=embed_type)\n","    # train_input_ids, train_sentences_encoding = extract_contextual_embedding(train_text.iloc[1:3], tokenizer, bert_model, max_len = max_len, low_RAM_inner_batch=False) test\n","    np.save('train_sentences_encoding.npy', train_sentences_encoding)\n","    np.save('train_input_ids.npy', train_input_ids)\n","\n","\n","dataloader, validation_dataloader = data_loader_BERT(train_sentences_encoding, train_input_ids, train_one_hot_labels, batch_size, random_state=1234, test_size=0.1)\n","del train_sentences_encoding, train_input_ids"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OJLMUVgVYema","executionInfo":{"status":"ok","timestamp":1639132152038,"user_tz":300,"elapsed":144169,"user":{"displayName":"Wei Yang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgKFRGCPHBXOIreYY1GgMyIUPCGnToBiaki3_u3=s64","userId":"18154280958911480081"}},"outputId":"408505a0-1a7b-47e6-d7bd-55877bafe3d5"},"id":"OJLMUVgVYema","execution_count":15,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 11083/11083 [01:20<00:00, 138.26it/s]\n"]}]},{"cell_type":"code","execution_count":16,"id":"6eb114c3","metadata":{"ExecuteTime":{"end_time":"2021-12-10T02:40:50.115095Z","start_time":"2021-12-10T02:40:50.102096Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"6eb114c3","executionInfo":{"status":"ok","timestamp":1639132152039,"user_tz":300,"elapsed":9,"user":{"displayName":"Wei Yang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgKFRGCPHBXOIreYY1GgMyIUPCGnToBiaki3_u3=s64","userId":"18154280958911480081"}},"outputId":"1f2fea70-7398-4f25-c242-800888c763b0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["clf_naive(\n","  (fc): Sequential(\n","    (0): Flatten(start_dim=1, end_dim=-1)\n","    (1): Linear(in_features=768, out_features=384, bias=True)\n","    (2): ReLU()\n","    (3): Dropout(p=0.0, inplace=False)\n","    (4): Linear(in_features=384, out_features=192, bias=True)\n","    (5): ReLU()\n","    (6): Dropout(p=0.0, inplace=False)\n","    (7): Linear(in_features=192, out_features=64, bias=True)\n","    (8): ReLU()\n","    (9): Dropout(p=0.0, inplace=False)\n","    (10): Linear(in_features=64, out_features=20, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":16}],"source":["model_path = 'bert_clf'\n","label_size = len(labels)\n","# model = lstm_cnn_o1(embed_dim, max_len, hidden_units, num_filters, kernel_sizes, label_size)\n","# model = lstm_cnn_o2(embed_dim, max_len, hidden_units, num_filters, kernel_sizes, label_size)\n","# model = clf(embed_dim, max_len, hidden_units, label_size)\n","model = clf_naive(embed_dim, max_len, hidden_units, label_size, dropout_rate=0.0)\n","model.to(device)"]},{"cell_type":"code","source":["[torch.cuda.empty_cache() for _ in range(10) ]\n","print(torch.cuda.memory_summary())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y75AD5TEun3D","executionInfo":{"status":"ok","timestamp":1639132152039,"user_tz":300,"elapsed":6,"user":{"displayName":"Wei Yang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgKFRGCPHBXOIreYY1GgMyIUPCGnToBiaki3_u3=s64","userId":"18154280958911480081"}},"outputId":"fdedd0e5-5e27-4d9b-98bc-1582693d656e"},"id":"y75AD5TEun3D","execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["|===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |  431171 KB |  463728 KB |    1465 GB |    1465 GB |\n","|       from large pool |  430336 KB |  436480 KB |     519 GB |     519 GB |\n","|       from small pool |     835 KB |   32369 KB |     946 GB |     946 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |  431171 KB |  463728 KB |    1465 GB |    1465 GB |\n","|       from large pool |  430336 KB |  436480 KB |     519 GB |     519 GB |\n","|       from small pool |     835 KB |   32369 KB |     946 GB |     946 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |  483328 KB |  516096 KB |  516096 KB |   32768 KB |\n","|       from large pool |  481280 KB |  481280 KB |  481280 KB |       0 KB |\n","|       from small pool |    2048 KB |   34816 KB |   34816 KB |   32768 KB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |   52156 KB |   71567 KB |    1513 GB |    1513 GB |\n","|       from large pool |   50944 KB |   52992 KB |     519 GB |     519 GB |\n","|       from small pool |    1212 KB |   19343 KB |     994 GB |     994 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     209    |     266    |    2948 K  |    2948 K  |\n","|       from large pool |      76    |      79    |     266 K  |     265 K  |\n","|       from small pool |     133    |     190    |    2682 K  |    2682 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     209    |     266    |    2948 K  |    2948 K  |\n","|       from large pool |      76    |      79    |     266 K  |     265 K  |\n","|       from small pool |     133    |     190    |    2682 K  |    2682 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      21    |      37    |      37    |      16    |\n","|       from large pool |      20    |      20    |      20    |       0    |\n","|       from small pool |       1    |      17    |      17    |      16    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      18    |      51    |    1994 K  |    1994 K  |\n","|       from large pool |      17    |      18    |     266 K  |     265 K  |\n","|       from small pool |       1    |      33    |    1728 K  |    1728 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n"]}]},{"cell_type":"code","execution_count":null,"id":"0f1f862a","metadata":{"ExecuteTime":{"end_time":"2021-12-10T02:41:35.524180Z","start_time":"2021-12-10T02:41:35.498181Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"0f1f862a","outputId":"fba3b1a8-95d8-423b-ccec-1e86d6241eeb"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n","\n","======== Epoch 1 / 80 ========\n","Training...\n","    Epoch 1\t Train Loss: 2.7542\t Train Acc: 0.3282\t Train F1: 0.2133\t Train ovr AUC: 0.8247\t Train precision: 0.2490\t Train recall: 0.2924\n","    Epoch 1\t Val Loss: 2.7613\t Val Acc: 0.3273\t Val F1: 0.2172\t Val ovr AUC: 0.8148\t Val precision: 0.2422\t Val recall: 0.2931\n","model saved\n","\n","======== Epoch 2 / 80 ========\n","Training...\n","    Epoch 2\t Train Loss: 2.6084\t Train Acc: 0.4748\t Train F1: 0.3638\t Train ovr AUC: 0.8680\t Train precision: 0.3402\t Train recall: 0.4358\n","    Epoch 2\t Val Loss: 2.6302\t Val Acc: 0.4500\t Val F1: 0.3435\t Val ovr AUC: 0.8601\t Val precision: 0.3257\t Val recall: 0.4132\n","model saved\n","\n","======== Epoch 3 / 80 ========\n","Training...\n","    Epoch 3\t Train Loss: 2.6013\t Train Acc: 0.4761\t Train F1: 0.3575\t Train ovr AUC: 0.8648\t Train precision: 0.3777\t Train recall: 0.4415\n","    Epoch 3\t Val Loss: 2.6293\t Val Acc: 0.4472\t Val F1: 0.3339\t Val ovr AUC: 0.8557\t Val precision: 0.3089\t Val recall: 0.4151\n","\n","======== Epoch 4 / 80 ========\n","Training...\n","    Epoch 4\t Train Loss: 2.6030\t Train Acc: 0.4735\t Train F1: 0.3618\t Train ovr AUC: 0.8466\t Train precision: 0.3460\t Train recall: 0.4347\n","    Epoch 4\t Val Loss: 2.6234\t Val Acc: 0.4536\t Val F1: 0.3483\t Val ovr AUC: 0.8380\t Val precision: 0.3402\t Val recall: 0.4181\n","\n","======== Epoch 5 / 80 ========\n","Training...\n","    Epoch 5\t Train Loss: 2.9033\t Train Acc: 0.1738\t Train F1: 0.1050\t Train ovr AUC: 0.6896\t Train precision: 0.2145\t Train recall: 0.1498\n","    Epoch 5\t Val Loss: 2.9063\t Val Acc: 0.1686\t Val F1: 0.0992\t Val ovr AUC: 0.6939\t Val precision: 0.2220\t Val recall: 0.1454\n","\n","======== Epoch 6 / 80 ========\n","Training...\n","    Epoch 6\t Train Loss: 3.0167\t Train Acc: 0.0615\t Train F1: 0.0160\t Train ovr AUC: 0.5085\t Train precision: 0.1533\t Train recall: 0.0558\n","    Epoch 6\t Val Loss: 3.0205\t Val Acc: 0.0568\t Val F1: 0.0092\t Val ovr AUC: 0.5066\t Val precision: 0.0599\t Val recall: 0.0521\n","\n","======== Epoch 7 / 80 ========\n","Training...\n","    Epoch 7\t Train Loss: 3.0242\t Train Acc: 0.0539\t Train F1: 0.0052\t Train ovr AUC: 0.5004\t Train precision: 0.0527\t Train recall: 0.0501\n","    Epoch 7\t Val Loss: 3.0232\t Val Acc: 0.0541\t Val F1: 0.0051\t Val ovr AUC: 0.5000\t Val precision: 0.0027\t Val recall: 0.0500\n","\n","======== Epoch 8 / 80 ========\n","Training...\n","    Epoch 8\t Train Loss: 3.0241\t Train Acc: 0.0539\t Train F1: 0.0052\t Train ovr AUC: 0.5004\t Train precision: 0.0527\t Train recall: 0.0501\n","    Epoch 8\t Val Loss: 3.0232\t Val Acc: 0.0541\t Val F1: 0.0051\t Val ovr AUC: 0.5000\t Val precision: 0.0027\t Val recall: 0.0500\n","\n","======== Epoch 9 / 80 ========\n","Training...\n","    Epoch 9\t Train Loss: 3.0273\t Train Acc: 0.0509\t Train F1: 0.0048\t Train ovr AUC: 0.5000\t Train precision: 0.0025\t Train recall: 0.0500\n","    Epoch 9\t Val Loss: 3.0282\t Val Acc: 0.0505\t Val F1: 0.0048\t Val ovr AUC: 0.5000\t Val precision: 0.0025\t Val recall: 0.0500\n","\n","======== Epoch 10 / 80 ========\n","Training...\n","    Epoch 10\t Train Loss: 3.0273\t Train Acc: 0.0509\t Train F1: 0.0048\t Train ovr AUC: 0.5000\t Train precision: 0.0025\t Train recall: 0.0500\n","    Epoch 10\t Val Loss: 3.0282\t Val Acc: 0.0505\t Val F1: 0.0048\t Val ovr AUC: 0.5000\t Val precision: 0.0025\t Val recall: 0.0500\n","\n","======== Epoch 11 / 80 ========\n","Training...\n","    Epoch 11\t Train Loss: 3.0272\t Train Acc: 0.0509\t Train F1: 0.0048\t Train ovr AUC: 0.5000\t Train precision: 0.0025\t Train recall: 0.0500\n","    Epoch 11\t Val Loss: 3.0282\t Val Acc: 0.0505\t Val F1: 0.0048\t Val ovr AUC: 0.5000\t Val precision: 0.0025\t Val recall: 0.0500\n","\n","======== Epoch 12 / 80 ========\n","Training...\n","    Epoch 12\t Train Loss: 3.0273\t Train Acc: 0.0509\t Train F1: 0.0048\t Train ovr AUC: 0.5000\t Train precision: 0.0025\t Train recall: 0.0500\n","    Epoch 12\t Val Loss: 3.0282\t Val Acc: 0.0505\t Val F1: 0.0048\t Val ovr AUC: 0.5000\t Val precision: 0.0025\t Val recall: 0.0500\n","\n","======== Epoch 13 / 80 ========\n","Training...\n","    Epoch 13\t Train Loss: 3.0272\t Train Acc: 0.0509\t Train F1: 0.0048\t Train ovr AUC: 0.5000\t Train precision: 0.0025\t Train recall: 0.0500\n","    Epoch 13\t Val Loss: 3.0282\t Val Acc: 0.0505\t Val F1: 0.0048\t Val ovr AUC: 0.5000\t Val precision: 0.0025\t Val recall: 0.0500\n","\n","======== Epoch 14 / 80 ========\n","Training...\n","    Epoch 14\t Train Loss: 3.0273\t Train Acc: 0.0509\t Train F1: 0.0048\t Train ovr AUC: 0.5000\t Train precision: 0.0025\t Train recall: 0.0500\n","    Epoch 14\t Val Loss: 3.0282\t Val Acc: 0.0505\t Val F1: 0.0048\t Val ovr AUC: 0.5000\t Val precision: 0.0025\t Val recall: 0.0500\n","\n","======== Epoch 15 / 80 ========\n","Training...\n","    Epoch 15\t Train Loss: 3.0272\t Train Acc: 0.0509\t Train F1: 0.0048\t Train ovr AUC: 0.5000\t Train precision: 0.0025\t Train recall: 0.0500\n","    Epoch 15\t Val Loss: 3.0282\t Val Acc: 0.0505\t Val F1: 0.0048\t Val ovr AUC: 0.5000\t Val precision: 0.0025\t Val recall: 0.0500\n","\n","======== Epoch 16 / 80 ========\n","Training...\n","    Epoch 16\t Train Loss: 3.0273\t Train Acc: 0.0509\t Train F1: 0.0048\t Train ovr AUC: 0.5000\t Train precision: 0.0025\t Train recall: 0.0500\n","    Epoch 16\t Val Loss: 3.0282\t Val Acc: 0.0505\t Val F1: 0.0048\t Val ovr AUC: 0.5000\t Val precision: 0.0025\t Val recall: 0.0500\n","\n","======== Epoch 17 / 80 ========\n","Training...\n","    Epoch 17\t Train Loss: 3.0272\t Train Acc: 0.0509\t Train F1: 0.0048\t Train ovr AUC: 0.5000\t Train precision: 0.0025\t Train recall: 0.0500\n","    Epoch 17\t Val Loss: 3.0282\t Val Acc: 0.0505\t Val F1: 0.0048\t Val ovr AUC: 0.5000\t Val precision: 0.0025\t Val recall: 0.0500\n","\n","======== Epoch 18 / 80 ========\n","Training...\n","    Epoch 18\t Train Loss: 3.0273\t Train Acc: 0.0509\t Train F1: 0.0048\t Train ovr AUC: 0.5000\t Train precision: 0.0025\t Train recall: 0.0500\n","    Epoch 18\t Val Loss: 3.0282\t Val Acc: 0.0505\t Val F1: 0.0048\t Val ovr AUC: 0.5000\t Val precision: 0.0025\t Val recall: 0.0500\n","\n","======== Epoch 19 / 80 ========\n","Training...\n","    Epoch 19\t Train Loss: 3.0272\t Train Acc: 0.0509\t Train F1: 0.0048\t Train ovr AUC: 0.5000\t Train precision: 0.0025\t Train recall: 0.0500\n","    Epoch 19\t Val Loss: 3.0282\t Val Acc: 0.0505\t Val F1: 0.0048\t Val ovr AUC: 0.5000\t Val precision: 0.0025\t Val recall: 0.0500\n","\n","======== Epoch 20 / 80 ========\n","Training...\n","    Epoch 20\t Train Loss: 3.0272\t Train Acc: 0.0509\t Train F1: 0.0048\t Train ovr AUC: 0.5000\t Train precision: 0.0025\t Train recall: 0.0500\n","    Epoch 20\t Val Loss: 3.0282\t Val Acc: 0.0505\t Val F1: 0.0048\t Val ovr AUC: 0.5000\t Val precision: 0.0025\t Val recall: 0.0500\n","\n","======== Epoch 21 / 80 ========\n","Training...\n","    Epoch 21\t Train Loss: 3.0273\t Train Acc: 0.0509\t Train F1: 0.0048\t Train ovr AUC: 0.5000\t Train precision: 0.0025\t Train recall: 0.0500\n","    Epoch 21\t Val Loss: 3.0282\t Val Acc: 0.0505\t Val F1: 0.0048\t Val ovr AUC: 0.5000\t Val precision: 0.0025\t Val recall: 0.0500\n","\n","======== Epoch 22 / 80 ========\n","Training...\n","    Epoch 22\t Train Loss: 3.0273\t Train Acc: 0.0509\t Train F1: 0.0048\t Train ovr AUC: 0.5000\t Train precision: 0.0025\t Train recall: 0.0500\n","    Epoch 22\t Val Loss: 3.0282\t Val Acc: 0.0505\t Val F1: 0.0048\t Val ovr AUC: 0.5000\t Val precision: 0.0025\t Val recall: 0.0500\n","\n","======== Epoch 23 / 80 ========\n","Training...\n","    Epoch 23\t Train Loss: 3.0273\t Train Acc: 0.0509\t Train F1: 0.0048\t Train ovr AUC: 0.5000\t Train precision: 0.0025\t Train recall: 0.0500\n","    Epoch 23\t Val Loss: 3.0282\t Val Acc: 0.0505\t Val F1: 0.0048\t Val ovr AUC: 0.5000\t Val precision: 0.0025\t Val recall: 0.0500\n","\n","======== Epoch 24 / 80 ========\n","Training...\n","    Epoch 24\t Train Loss: 3.0272\t Train Acc: 0.0509\t Train F1: 0.0048\t Train ovr AUC: 0.5000\t Train precision: 0.0025\t Train recall: 0.0500\n","    Epoch 24\t Val Loss: 3.0282\t Val Acc: 0.0505\t Val F1: 0.0048\t Val ovr AUC: 0.5000\t Val precision: 0.0025\t Val recall: 0.0500\n","\n","======== Epoch 25 / 80 ========\n","Training...\n","    Epoch 25\t Train Loss: 3.0273\t Train Acc: 0.0509\t Train F1: 0.0048\t Train ovr AUC: 0.5000\t Train precision: 0.0025\t Train recall: 0.0500\n","    Epoch 25\t Val Loss: 3.0282\t Val Acc: 0.0505\t Val F1: 0.0048\t Val ovr AUC: 0.5000\t Val precision: 0.0025\t Val recall: 0.0500\n","\n","======== Epoch 26 / 80 ========\n","Training...\n","    Epoch 26\t Train Loss: 3.0272\t Train Acc: 0.0509\t Train F1: 0.0048\t Train ovr AUC: 0.5000\t Train precision: 0.0025\t Train recall: 0.0500\n","    Epoch 26\t Val Loss: 3.0282\t Val Acc: 0.0505\t Val F1: 0.0048\t Val ovr AUC: 0.5000\t Val precision: 0.0025\t Val recall: 0.0500\n","\n","======== Epoch 27 / 80 ========\n","Training...\n","    Epoch 27\t Train Loss: 3.0273\t Train Acc: 0.0509\t Train F1: 0.0048\t Train ovr AUC: 0.5000\t Train precision: 0.0025\t Train recall: 0.0500\n","    Epoch 27\t Val Loss: 3.0282\t Val Acc: 0.0505\t Val F1: 0.0048\t Val ovr AUC: 0.5000\t Val precision: 0.0025\t Val recall: 0.0500\n","\n","======== Epoch 28 / 80 ========\n","Training...\n","    Epoch 28\t Train Loss: 3.0272\t Train Acc: 0.0509\t Train F1: 0.0048\t Train ovr AUC: 0.5000\t Train precision: 0.0025\t Train recall: 0.0500\n","    Epoch 28\t Val Loss: 3.0282\t Val Acc: 0.0505\t Val F1: 0.0048\t Val ovr AUC: 0.5000\t Val precision: 0.0025\t Val recall: 0.0500\n","\n","======== Epoch 29 / 80 ========\n","Training...\n","    Epoch 29\t Train Loss: 3.0273\t Train Acc: 0.0509\t Train F1: 0.0048\t Train ovr AUC: 0.5000\t Train precision: 0.0025\t Train recall: 0.0500\n","    Epoch 29\t Val Loss: 3.0282\t Val Acc: 0.0505\t Val F1: 0.0048\t Val ovr AUC: 0.5000\t Val precision: 0.0025\t Val recall: 0.0500\n","\n","======== Epoch 30 / 80 ========\n","Training...\n","    Epoch 30\t Train Loss: 3.0272\t Train Acc: 0.0509\t Train F1: 0.0048\t Train ovr AUC: 0.5000\t Train precision: 0.0025\t Train recall: 0.0500\n","    Epoch 30\t Val Loss: 3.0282\t Val Acc: 0.0505\t Val F1: 0.0048\t Val ovr AUC: 0.5000\t Val precision: 0.0025\t Val recall: 0.0500\n","\n","======== Epoch 31 / 80 ========\n","Training...\n","    Epoch 31\t Train Loss: 3.0271\t Train Acc: 0.0509\t Train F1: 0.0048\t Train ovr AUC: 0.5000\t Train precision: 0.0025\t Train recall: 0.0500\n","    Epoch 31\t Val Loss: 3.0282\t Val Acc: 0.0505\t Val F1: 0.0048\t Val ovr AUC: 0.5000\t Val precision: 0.0025\t Val recall: 0.0500\n","\n","======== Epoch 32 / 80 ========\n","Training...\n","    Epoch 32\t Train Loss: 3.0273\t Train Acc: 0.0509\t Train F1: 0.0048\t Train ovr AUC: 0.5000\t Train precision: 0.0025\t Train recall: 0.0500\n","    Epoch 32\t Val Loss: 3.0282\t Val Acc: 0.0505\t Val F1: 0.0048\t Val ovr AUC: 0.5000\t Val precision: 0.0025\t Val recall: 0.0500\n","\n","======== Epoch 33 / 80 ========\n","Training...\n","    Epoch 33\t Train Loss: 3.0273\t Train Acc: 0.0509\t Train F1: 0.0048\t Train ovr AUC: 0.5000\t Train precision: 0.0025\t Train recall: 0.0500\n","    Epoch 33\t Val Loss: 3.0282\t Val Acc: 0.0505\t Val F1: 0.0048\t Val ovr AUC: 0.5000\t Val precision: 0.0025\t Val recall: 0.0500\n","\n","======== Epoch 34 / 80 ========\n","Training...\n","    Epoch 34\t Train Loss: 3.0272\t Train Acc: 0.0509\t Train F1: 0.0048\t Train ovr AUC: 0.5000\t Train precision: 0.0025\t Train recall: 0.0500\n","    Epoch 34\t Val Loss: 3.0282\t Val Acc: 0.0505\t Val F1: 0.0048\t Val ovr AUC: 0.5000\t Val precision: 0.0025\t Val recall: 0.0500\n","\n","======== Epoch 35 / 80 ========\n","Training...\n","    Epoch 35\t Train Loss: 3.0271\t Train Acc: 0.0509\t Train F1: 0.0048\t Train ovr AUC: 0.5000\t Train precision: 0.0025\t Train recall: 0.0500\n","    Epoch 35\t Val Loss: 3.0282\t Val Acc: 0.0505\t Val F1: 0.0048\t Val ovr AUC: 0.5000\t Val precision: 0.0025\t Val recall: 0.0500\n","\n","======== Epoch 36 / 80 ========\n","Training...\n","    Epoch 36\t Train Loss: 3.0272\t Train Acc: 0.0509\t Train F1: 0.0048\t Train ovr AUC: 0.5000\t Train precision: 0.0025\t Train recall: 0.0500\n","    Epoch 36\t Val Loss: 3.0282\t Val Acc: 0.0505\t Val F1: 0.0048\t Val ovr AUC: 0.5000\t Val precision: 0.0025\t Val recall: 0.0500\n","\n","======== Epoch 37 / 80 ========\n","Training...\n","    Epoch 37\t Train Loss: 3.0273\t Train Acc: 0.0509\t Train F1: 0.0048\t Train ovr AUC: 0.5000\t Train precision: 0.0025\t Train recall: 0.0500\n","    Epoch 37\t Val Loss: 3.0282\t Val Acc: 0.0505\t Val F1: 0.0048\t Val ovr AUC: 0.5000\t Val precision: 0.0025\t Val recall: 0.0500\n","\n","======== Epoch 38 / 80 ========\n","Training...\n","    Epoch 38\t Train Loss: 3.0272\t Train Acc: 0.0509\t Train F1: 0.0048\t Train ovr AUC: 0.5000\t Train precision: 0.0025\t Train recall: 0.0500\n","    Epoch 38\t Val Loss: 3.0282\t Val Acc: 0.0505\t Val F1: 0.0048\t Val ovr AUC: 0.5000\t Val precision: 0.0025\t Val recall: 0.0500\n","\n","======== Epoch 39 / 80 ========\n","Training...\n","    Epoch 39\t Train Loss: 3.0272\t Train Acc: 0.0509\t Train F1: 0.0048\t Train ovr AUC: 0.5000\t Train precision: 0.0025\t Train recall: 0.0500\n","    Epoch 39\t Val Loss: 3.0282\t Val Acc: 0.0505\t Val F1: 0.0048\t Val ovr AUC: 0.5000\t Val precision: 0.0025\t Val recall: 0.0500\n","\n","======== Epoch 40 / 80 ========\n","Training...\n","    Epoch 40\t Train Loss: 3.0273\t Train Acc: 0.0509\t Train F1: 0.0048\t Train ovr AUC: 0.5000\t Train precision: 0.0025\t Train recall: 0.0500\n","    Epoch 40\t Val Loss: 3.0282\t Val Acc: 0.0505\t Val F1: 0.0048\t Val ovr AUC: 0.5000\t Val precision: 0.0025\t Val recall: 0.0500\n","\n","======== Epoch 41 / 80 ========\n","Training...\n","    Epoch 41\t Train Loss: 3.0273\t Train Acc: 0.0509\t Train F1: 0.0048\t Train ovr AUC: 0.5000\t Train precision: 0.0025\t Train recall: 0.0500\n","    Epoch 41\t Val Loss: 3.0282\t Val Acc: 0.0505\t Val F1: 0.0048\t Val ovr AUC: 0.5000\t Val precision: 0.0025\t Val recall: 0.0500\n","\n","======== Epoch 42 / 80 ========\n","Training...\n","    Epoch 42\t Train Loss: 3.0272\t Train Acc: 0.0509\t Train F1: 0.0048\t Train ovr AUC: 0.5000\t Train precision: 0.0025\t Train recall: 0.0500\n","    Epoch 42\t Val Loss: 3.0282\t Val Acc: 0.0505\t Val F1: 0.0048\t Val ovr AUC: 0.5000\t Val precision: 0.0025\t Val recall: 0.0500\n","\n","======== Epoch 43 / 80 ========\n","Training...\n","    Epoch 43\t Train Loss: 3.0273\t Train Acc: 0.0509\t Train F1: 0.0048\t Train ovr AUC: 0.5000\t Train precision: 0.0025\t Train recall: 0.0500\n","    Epoch 43\t Val Loss: 3.0282\t Val Acc: 0.0505\t Val F1: 0.0048\t Val ovr AUC: 0.5000\t Val precision: 0.0025\t Val recall: 0.0500\n","\n","======== Epoch 44 / 80 ========\n","Training...\n","    Epoch 44\t Train Loss: 3.0273\t Train Acc: 0.0509\t Train F1: 0.0048\t Train ovr AUC: 0.5000\t Train precision: 0.0025\t Train recall: 0.0500\n","    Epoch 44\t Val Loss: 3.0282\t Val Acc: 0.0505\t Val F1: 0.0048\t Val ovr AUC: 0.5000\t Val precision: 0.0025\t Val recall: 0.0500\n","\n","======== Epoch 45 / 80 ========\n","Training...\n","    Epoch 45\t Train Loss: 3.0272\t Train Acc: 0.0509\t Train F1: 0.0048\t Train ovr AUC: 0.5000\t Train precision: 0.0025\t Train recall: 0.0500\n","    Epoch 45\t Val Loss: 3.0282\t Val Acc: 0.0505\t Val F1: 0.0048\t Val ovr AUC: 0.5000\t Val precision: 0.0025\t Val recall: 0.0500\n","\n","======== Epoch 46 / 80 ========\n","Training...\n","    Epoch 46\t Train Loss: 3.0271\t Train Acc: 0.0509\t Train F1: 0.0048\t Train ovr AUC: 0.5000\t Train precision: 0.0025\t Train recall: 0.0500\n","    Epoch 46\t Val Loss: 3.0282\t Val Acc: 0.0505\t Val F1: 0.0048\t Val ovr AUC: 0.5000\t Val precision: 0.0025\t Val recall: 0.0500\n","\n","======== Epoch 47 / 80 ========\n","Training...\n","    Epoch 47\t Train Loss: 3.0272\t Train Acc: 0.0509\t Train F1: 0.0048\t Train ovr AUC: 0.5000\t Train precision: 0.0025\t Train recall: 0.0500\n","    Epoch 47\t Val Loss: 3.0282\t Val Acc: 0.0505\t Val F1: 0.0048\t Val ovr AUC: 0.5000\t Val precision: 0.0025\t Val recall: 0.0500\n","\n","======== Epoch 48 / 80 ========\n","Training...\n","    Epoch 48\t Train Loss: 3.0273\t Train Acc: 0.0509\t Train F1: 0.0048\t Train ovr AUC: 0.5000\t Train precision: 0.0025\t Train recall: 0.0500\n","    Epoch 48\t Val Loss: 3.0282\t Val Acc: 0.0505\t Val F1: 0.0048\t Val ovr AUC: 0.5000\t Val precision: 0.0025\t Val recall: 0.0500\n","\n","======== Epoch 49 / 80 ========\n","Training...\n","    Epoch 49\t Train Loss: 3.0273\t Train Acc: 0.0509\t Train F1: 0.0048\t Train ovr AUC: 0.5000\t Train precision: 0.0025\t Train recall: 0.0500\n","    Epoch 49\t Val Loss: 3.0282\t Val Acc: 0.0505\t Val F1: 0.0048\t Val ovr AUC: 0.5000\t Val precision: 0.0025\t Val recall: 0.0500\n","\n","======== Epoch 50 / 80 ========\n","Training...\n","    Epoch 50\t Train Loss: 3.0273\t Train Acc: 0.0509\t Train F1: 0.0048\t Train ovr AUC: 0.5000\t Train precision: 0.0025\t Train recall: 0.0500\n","    Epoch 50\t Val Loss: 3.0282\t Val Acc: 0.0505\t Val F1: 0.0048\t Val ovr AUC: 0.5000\t Val precision: 0.0025\t Val recall: 0.0500\n","\n","======== Epoch 51 / 80 ========\n","Training...\n","    Epoch 51\t Train Loss: 3.0271\t Train Acc: 0.0509\t Train F1: 0.0048\t Train ovr AUC: 0.5000\t Train precision: 0.0025\t Train recall: 0.0500\n","    Epoch 51\t Val Loss: 3.0282\t Val Acc: 0.0505\t Val F1: 0.0048\t Val ovr AUC: 0.5000\t Val precision: 0.0025\t Val recall: 0.0500\n","\n","======== Epoch 52 / 80 ========\n","Training...\n","    Epoch 52\t Train Loss: 3.0272\t Train Acc: 0.0509\t Train F1: 0.0048\t Train ovr AUC: 0.5000\t Train precision: 0.0025\t Train recall: 0.0500\n","    Epoch 52\t Val Loss: 3.0282\t Val Acc: 0.0505\t Val F1: 0.0048\t Val ovr AUC: 0.5000\t Val precision: 0.0025\t Val recall: 0.0500\n","\n","======== Epoch 53 / 80 ========\n","Training...\n","    Epoch 53\t Train Loss: 3.0273\t Train Acc: 0.0509\t Train F1: 0.0048\t Train ovr AUC: 0.5000\t Train precision: 0.0025\t Train recall: 0.0500\n","    Epoch 53\t Val Loss: 3.0282\t Val Acc: 0.0505\t Val F1: 0.0048\t Val ovr AUC: 0.5000\t Val precision: 0.0025\t Val recall: 0.0500\n","\n","======== Epoch 54 / 80 ========\n","Training...\n","    Epoch 54\t Train Loss: 3.0273\t Train Acc: 0.0509\t Train F1: 0.0048\t Train ovr AUC: 0.5000\t Train precision: 0.0025\t Train recall: 0.0500\n","    Epoch 54\t Val Loss: 3.0282\t Val Acc: 0.0505\t Val F1: 0.0048\t Val ovr AUC: 0.5000\t Val precision: 0.0025\t Val recall: 0.0500\n","\n","======== Epoch 55 / 80 ========\n","Training...\n","    Epoch 55\t Train Loss: 3.0272\t Train Acc: 0.0509\t Train F1: 0.0048\t Train ovr AUC: 0.5000\t Train precision: 0.0025\t Train recall: 0.0500\n","    Epoch 55\t Val Loss: 3.0282\t Val Acc: 0.0505\t Val F1: 0.0048\t Val ovr AUC: 0.5000\t Val precision: 0.0025\t Val recall: 0.0500\n","\n","======== Epoch 56 / 80 ========\n","Training...\n","    Epoch 56\t Train Loss: 3.0271\t Train Acc: 0.0509\t Train F1: 0.0048\t Train ovr AUC: 0.5000\t Train precision: 0.0025\t Train recall: 0.0500\n","    Epoch 56\t Val Loss: 3.0282\t Val Acc: 0.0505\t Val F1: 0.0048\t Val ovr AUC: 0.5000\t Val precision: 0.0025\t Val recall: 0.0500\n","\n","======== Epoch 57 / 80 ========\n","Training...\n"]}],"source":["# Data Loader\n","\n","optimizer = AdamW(model.parameters(), lr=learning_rate, eps=1e-8)\n","total_steps = len(dataloader) * epochs\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(total_steps * 0.1), num_training_steps=total_steps)\n","model, training_stats, pred_labels, true_labels = train_multi_label_model(model, label_size, label_cols, dataloader, validation_dataloader, optimizer=optimizer, scheduler=scheduler, epochs=epochs, patience=patience, model_path=model_path)\n","\n","\n","pd.set_option('precision', 2)\n","df_stats = pd.DataFrame(data=training_stats)\n","df_stats = df_stats.set_index('epoch')\n","df_stats.to_csv(model_path[0:-2] + 'csv')\n","\n","import seaborn as sns\n","sns.set(style='darkgrid')\n","sns.set(font_scale=1.5)\n","plt.rcParams[\"figure.figsize\"] = (12, 6)\n","plt.plot(df_stats['train_loss'], 'b-o', label=\"Training\")\n","plt.plot(df_stats['val_loss'], 'g-o', label=\"Validation\")\n","plt.legend()\n","plt.title(\"Training & Validation Loss\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.xticks(list(range(1, epochs + 1)))\n","plt.show()"]},{"cell_type":"code","source":["# model.load_state_dict(torch.load(model_path))"],"metadata":{"id":"flba59bywKcr"},"id":"flba59bywKcr","execution_count":null,"outputs":[]},{"cell_type":"code","source":["if load_embed is True:\n","    test_sentences_encoding = np.load('test_sentences_encoding.npy')#, mmap_mode='r')\n","    test_input_ids = np.load('test_input_ids.npy') # , mmap_mode='r')\n","    test_sentences_encoding = torch.tensor(test_sentences_encoding)\n","    test_input_ids = torch.tensor(test_input_ids)\n","else:\n","    test_input_ids, test_sentences_encoding = extract_contextual_embedding(test_text, tokenizer, bert_model, max_len = max_len, low_RAM_inner_batch=True, embed_type=embed_type)\n","    np.save('test_sentences_encoding.npy', test_sentences_encoding)\n","    np.save('test_input_ids.npy', test_input_ids)\n","\n","\n","test_dataloader, _ = data_loader_BERT(test_sentences_encoding,test_input_ids, test_one_hot_labels, testing=True)\n","del test_sentences_encoding, test_input_ids\n","\n","tokenized_texts, pred_labels, true_labels, avg_val_loss, auc_score, precison, recall, acc, f1 = model_eval(model, test_dataloader,  labels , class_weight=None)\n","classification_report = evaluation_report(np.argmax(true_labels, axis=1),  np.argmax(pred_labels, axis=1), labels=labels)\n","roc_auc(y_test, pred_labels)"],"metadata":{"id":"z7axjlu1jF7u"},"id":"z7axjlu1jF7u","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"prcAg6kJ2KbK"},"id":"prcAg6kJ2KbK","execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_stats"],"metadata":{"id":"BpIW3VT72KdX"},"id":"BpIW3VT72KdX","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"ea5b_TL62Y2a"},"id":"ea5b_TL62Y2a","execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"NN_based_models_BERT_v1_text.ipynb","provenance":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"},"latex_envs":{"LaTeX_envs_menu_present":true,"autoclose":false,"autocomplete":true,"bibliofile":"biblio.bib","cite_by":"apalike","current_citInitial":1,"eqLabelWithNumbers":true,"eqNumInitial":1,"hotkeys":{"equation":"Ctrl-E","itemize":"Ctrl-I"},"labels_anchors":false,"latex_user_defs":false,"report_style_numbering":false,"user_envs_cfg":false},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":true,"toc_position":{"height":"calc(100% - 180px)","left":"10px","top":"150px","width":"190.458px"},"toc_section_display":true,"toc_window_display":true}},"nbformat":4,"nbformat_minor":5}