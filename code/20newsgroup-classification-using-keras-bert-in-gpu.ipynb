{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will use Keras-BERT and adapters available in keras for finetuning.\n","metadata":{}},{"cell_type":"code","source":"!pip install keras-bert\n!pip install keras-rectified-adam\n\n!wget -q https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n!unzip -o uncased_L-12_H-768_A-12.zip","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import codecs\nimport tensorflow as tf\nfrom tqdm import tqdm\nfrom chardet import detect\nimport keras\nfrom keras_radam import RAdam\nfrom keras import backend as K\nfrom keras_bert import load_trained_model_from_checkpoint\n# from google.colab import drive\n\n","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**PARAMETERS**","metadata":{}},{"cell_type":"code","source":"\nSEQ_LEN = 128\nBATCH_SIZE = 50\nEPOCHS = 7\nLR = 1e-4","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Path to the pre trained model of BERT.**","metadata":{}},{"cell_type":"code","source":"\n\npretrained_path = 'uncased_L-12_H-768_A-12'\nconfig_path = os.path.join(pretrained_path, 'bert_config.json')\ncheckpoint_path = os.path.join(pretrained_path, 'bert_model.ckpt')\nvocab_path = os.path.join(pretrained_path, 'vocab.txt')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Loading Pretrained BERT model.**","metadata":{}},{"cell_type":"code","source":"model = load_trained_model_from_checkpoint(\n      config_path,\n      checkpoint_path,\n      training=True,\n      trainable=True,\n      seq_len=SEQ_LEN,\n  )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Extracting token dictionary from vocab of pretrained model to refer for input we will be using.**","metadata":{}},{"cell_type":"code","source":"import codecs\nfrom keras_bert import Tokenizer\ntoken_dict = {}\nwith codecs.open(vocab_path, 'r', 'utf8') as reader:\n    for line in reader:\n        token = line.strip()\n        token_dict[token] = len(token_dict)\n        \n# print(token_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Downloading dataset.**","metadata":{}},{"cell_type":"code","source":"# @title Download I20Newsgroup dataset\nimport tensorflow as tf\n\ndataset = tf.keras.utils.get_file(\n    fname=\"20news-18828.tar.gz\", \n    origin=\"http://qwone.com/~jason/20Newsgroups/20news-18828.tar.gz\", \n    extract=True,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Defining Tokenizer.**","metadata":{}},{"cell_type":"code","source":"tokenizer = Tokenizer(token_dict)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Making Label , index pair.**","metadata":{}},{"cell_type":"code","source":"datapath = \".\".join(dataset.split(\".\")[:-2])\ntxtfiles = os.listdir(datapath)\nlabels = [(x, i) for i,x in enumerate(txtfiles)]\ndef get_label(index):\n    for each in labels:\n        if index == each[1]:\n            return each[0]\n    \n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Splitting data into train, test preceded by tokenizing sentences and returning test train data. **","metadata":{}},{"cell_type":"code","source":"def load_data(path, labels):\n    global tokenizer\n    indices, sentiments = [], []\n    for folder, sentiment in labels:\n        folder = os.path.join(path, folder)\n        for name in tqdm(os.listdir(folder)):\n            with open(os.path.join(folder, name), 'r', encoding=\"utf-8\", errors='ignore') as reader:\n                  text = reader.read()\n            ids, segments = tokenizer.encode(text, max_len=SEQ_LEN)\n            indices.append(ids)\n            sentiments.append(sentiment)\n    items = list(zip(indices, sentiments))\n    \n    np.random.shuffle(items)\n    test_items = items[int(0.8*len(items)):]\n    train_items = items[:int(0.8*len(items))]\n    indices_test, sentiments_test = zip(*test_items)\n    indices_train, sentiments_train = zip(*train_items)\n    indices_train = np.array(indices_train)\n    indices_test = np.array(indices_test)\n    mod_train = indices_train.shape[0] % BATCH_SIZE\n    mod_test = indices_test.shape[0] % BATCH_SIZE\n    if mod_train > 0:\n        indices_train, sentiments_train = indices_train[:-mod_train], sentiments_train[:-mod_train]\n    if mod_test > 0:\n      indices_test, sentiments_test = indices_test[:-mod_test], sentiments_test[:-mod_test]\n\n    return [indices_train, np.zeros_like(indices_train)], np.array(sentiments_train),[indices_test, np.zeros_like(indices_test)], np.array(sentiments_test)\n  \ntrain_path = os.path.join(os.path.dirname(dataset), '20news-18828')\ntrain_x, train_y, test_x, test_y = load_data(train_path, labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.Series(train_y).value_counts().plot(kind = 'bar')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.Series(test_y).value_counts().plot(kind = 'bar')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Extracting layer from pretrained bert model and adding a layer with softmax function to classify 20 classes of news.**","metadata":{}},{"cell_type":"code","source":"inputs = model.inputs[:2]\ndense = model.get_layer('NSP-Dense').output\noutputs = keras.layers.Dense(units=20, activation='softmax')(dense)\n\nmodel = keras.models.Model(inputs, outputs)\nmodel.compile(\n  RAdam(learning_rate =LR),\n  loss='sparse_categorical_crossentropy',\n  metrics=['sparse_categorical_accuracy'],\n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Initializing variables.**","metadata":{}},{"cell_type":"code","source":"sess = K.get_session()\nuninitialized_variables = set([i.decode('ascii') for i in sess.run(tf.report_uninitialized_variables())])\ninit_op = tf.variables_initializer(\n    [v for v in tf.global_variables() if v.name.split(':')[0] in uninitialized_variables]\n)\nsess.run(init_op)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Finally, training the model.**","metadata":{}},{"cell_type":"code","source":"# @title Fit\n\nmodel.fit(\n    train_x,\n    train_y,\n    epochs=EPOCHS,\n    batch_size=BATCH_SIZE,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Making prediction for test dataset.**","metadata":{}},{"cell_type":"code","source":"\n\npredicts = model.predict(test_x, verbose=True).argmax(axis=-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Calculating accuracy.**","metadata":{}},{"cell_type":"code","source":"\nprint(np.sum(test_y == predicts) / test_y.shape[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Inference**","metadata":{}},{"cell_type":"code","source":"\ntest_text = \"\"\"The Mumbai batsman is set to replace underperforming KL Rahul as an opener in upcoming Test series against South Africa in home conditions.India’s newly-appointed batting coach Vikram Rathour feels opener Rohit Sharma is “too good a player” to not be playing in all three formats. Rathour, like many former cricketers, backed Rohit to open for India in Test cricket.“He is too good a player to not be playing in any game. That is what is everyone is thinking. He has done so well in white-ball cricket as an opener so there is no reason why he can’t succeed as a Test opener provided he gets enough opportunities,” Rathour believes Rohit can be an asset to his team if he does good against South Africa in Tests.\"\"\"\ntest = \"\"\"Senate Democrats are planning to hold the floor on Tuesday evening for an hours-long talk-a-thon on the issue of gun violence.The floor marathon comes as the White House is struggling to find a place to land in the weeks-long debate over potential gun-law reforms.“Many of my colleagues have seen their communities torn apart by gun violence; some by horrific mass shootings, others by a relentless, daily stream. Many of them have worked for years to bring commonsense gun safety measures before the Senate,” Senate Minority Leader Charles Schumer (D-N.Y.) said Tuesday, in announcing the plan from the Senate floor.\"\"\"\nids, segments = tokenizer.encode(test, max_len=SEQ_LEN)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inpu = np.array(ids).reshape([1, SEQ_LEN])\nget_label(model.predict([inpu,np.zeros_like(inpu)]).argmax(axis=-1)[0])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"ids, segments = tokenizer.encode(test_text, max_len=SEQ_LEN)\ninpu = np.array(ids).reshape([1, SEQ_LEN])\nget_label(model.predict([inpu,np.zeros_like(inpu)]).argmax(axis=-1)[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}